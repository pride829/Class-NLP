{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 08: Phrase Classification\n",
    "The assignment this week needs you to distinguish between good and bad phrases of the word \"**earn**\" (e.g., earn money). The method, word2vector, learned today will be used in the process. \n",
    "\n",
    "There're some data for this assignment: \n",
    "* train.tsv: Some phrases with labels to train and validate the classification model. There are only two types of label: 1 means *good*; 0 means *bad*.\n",
    "* test.tsv: Same format as train.tsv. It's used to test your model.\n",
    "* GoogleNews-vectors-negative300.bin.gz: a pre-trained word2vector model trained by Google ([source](https://code.google.com/archive/p/word2vec/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirement\n",
    "* pandas\n",
    "* tensorflow\n",
    "* sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data\n",
    "We use dataframe to store data here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def loadData(path):\n",
    "    ngram = []\n",
    "    _class = []\n",
    "    with open(path) as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip(\"\\n\").split(\"\\t\")\n",
    "            ngram.append(line[0])\n",
    "            _class.append(int(line[1]))\n",
    "    return pd.DataFrame({\"phrase\":ngram,\"class\":_class})\n",
    "train = loadData(\"train.tsv\")\n",
    "test = loadData(\"test.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load word2vec model\n",
    "<font color=\"red\">**[ TODO ]**</font> Please load [GoogleNews-vectors-negative300.bin.gz](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g) model and check the embedding of the word `language`.\n",
    "\n",
    "* package `gensim` is a good choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.models.keyedvectors.KeyedVectors'>\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "kv = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "print(type(kv))\n",
    "#print(kv['language'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "for word, vector in zip(list(kv.index_to_key), kv.vectors):\n",
    "    coefs = np.asarray(vector, dtype='float32')\n",
    "    embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.30712891e-02,  1.68457031e-02,  1.54296875e-01,  1.27929688e-01,\n",
       "       -2.67578125e-01,  3.51562500e-02,  1.19140625e-01,  2.48046875e-01,\n",
       "        1.93359375e-01, -7.95898438e-02,  1.46484375e-01, -1.43554688e-01,\n",
       "       -3.04687500e-01,  3.46679688e-02, -1.85546875e-02,  1.06933594e-01,\n",
       "       -1.52343750e-01,  2.89062500e-01,  2.35595703e-02, -3.80859375e-01,\n",
       "        1.09863281e-01,  4.41406250e-01,  3.75976562e-02, -1.22680664e-02,\n",
       "        1.62353516e-02, -2.24609375e-01,  7.61718750e-02, -3.12500000e-02,\n",
       "       -2.16064453e-02,  1.49414062e-01, -4.02832031e-02, -4.46777344e-02,\n",
       "       -1.72851562e-01,  3.32031250e-02,  1.50390625e-01, -5.05371094e-02,\n",
       "        2.72216797e-02,  3.00781250e-01, -1.33789062e-01, -7.56835938e-02,\n",
       "        1.93359375e-01, -1.98242188e-01, -1.27563477e-02,  4.19921875e-01,\n",
       "       -2.19726562e-01,  1.44531250e-01, -3.93066406e-02,  1.94335938e-01,\n",
       "       -3.12500000e-01,  1.84570312e-01,  1.48773193e-04, -1.67968750e-01,\n",
       "       -7.37304688e-02, -3.12500000e-02,  1.57226562e-01,  3.30078125e-01,\n",
       "       -1.42578125e-01, -3.16406250e-01, -7.32421875e-02, -5.76171875e-02,\n",
       "        1.02050781e-01, -1.08886719e-01,  1.24023438e-01, -2.50244141e-02,\n",
       "       -2.49023438e-01,  1.25976562e-01, -1.79687500e-01,  3.32031250e-01,\n",
       "        7.14111328e-03,  2.51953125e-01,  4.34570312e-02, -4.34570312e-02,\n",
       "       -3.90625000e-01,  1.76757812e-01, -1.13525391e-02, -1.97753906e-02,\n",
       "        2.79296875e-01,  2.36328125e-01,  1.19140625e-01,  5.59082031e-02,\n",
       "        1.73828125e-01, -1.10839844e-01, -4.95605469e-02,  2.13867188e-01,\n",
       "        6.17675781e-02,  1.38671875e-01, -4.45556641e-03,  2.55859375e-01,\n",
       "        1.80664062e-01,  5.88378906e-02, -6.59179688e-02, -2.08007812e-01,\n",
       "       -1.19140625e-01, -1.57226562e-01,  5.02929688e-02, -6.29882812e-02,\n",
       "        5.00488281e-02, -7.27539062e-02,  1.74560547e-02, -3.56445312e-02,\n",
       "       -1.93359375e-01,  3.93066406e-02, -3.36914062e-02, -1.07421875e-01,\n",
       "        5.78613281e-02, -8.20312500e-02,  1.74560547e-02, -1.65039062e-01,\n",
       "        1.46484375e-01, -3.08837891e-02, -3.86718750e-01,  2.49023438e-01,\n",
       "        8.74023438e-02, -2.15820312e-01, -4.10156250e-02,  1.60156250e-01,\n",
       "        1.85546875e-01, -2.27050781e-02, -3.73535156e-02,  7.86132812e-02,\n",
       "       -1.46484375e-01,  6.78710938e-02,  1.26953125e-01,  3.30078125e-01,\n",
       "        1.11328125e-01,  9.27734375e-02, -3.45703125e-01, -1.41601562e-01,\n",
       "       -5.29785156e-02, -1.50390625e-01, -7.81250000e-02, -1.27929688e-01,\n",
       "       -4.02343750e-01, -1.41601562e-01,  8.44726562e-02,  1.08398438e-01,\n",
       "       -4.44335938e-02,  3.73535156e-02,  5.61523438e-02, -1.91406250e-01,\n",
       "        1.54296875e-01, -5.12695312e-02, -6.49414062e-02, -8.30078125e-02,\n",
       "        7.17773438e-02, -1.33789062e-01,  1.05468750e-01,  3.33984375e-01,\n",
       "       -1.08398438e-01,  1.91650391e-02,  2.14843750e-01,  2.15820312e-01,\n",
       "       -1.05468750e-01, -1.44531250e-01,  4.32128906e-02, -2.71484375e-01,\n",
       "       -3.78906250e-01,  1.09863281e-01, -8.15429688e-02, -6.12792969e-02,\n",
       "       -1.33789062e-01,  9.71679688e-02, -1.04370117e-02, -1.21093750e-01,\n",
       "       -2.44140625e-01,  1.02050781e-01,  1.10839844e-01, -1.00585938e-01,\n",
       "        1.71875000e-01, -3.61328125e-02, -4.39453125e-02,  2.83203125e-01,\n",
       "       -8.93554688e-02, -1.70898438e-01,  2.46093750e-01,  1.16699219e-01,\n",
       "        8.39843750e-02, -1.32812500e-01, -1.61132812e-01, -1.39648438e-01,\n",
       "       -8.59375000e-02, -1.37695312e-01, -9.32617188e-02, -1.33789062e-01,\n",
       "        1.65039062e-01,  4.93164062e-02, -1.21093750e-01, -2.11914062e-01,\n",
       "        1.61132812e-01, -1.07421875e-01, -3.97949219e-02, -3.51562500e-01,\n",
       "       -5.02929688e-02,  1.46484375e-01, -4.68750000e-02,  4.17480469e-02,\n",
       "       -1.27929688e-01, -9.76562500e-02, -2.46093750e-01,  6.78710938e-02,\n",
       "       -2.30468750e-01,  1.80664062e-02,  3.54003906e-02,  7.32421875e-02,\n",
       "       -2.23632812e-01, -1.25976562e-01,  2.12890625e-01, -3.93066406e-02,\n",
       "       -2.41699219e-02, -9.61914062e-02,  7.51953125e-02, -1.46484375e-01,\n",
       "       -1.49414062e-01, -8.83789062e-02, -4.88281250e-02,  2.32421875e-01,\n",
       "        3.30078125e-01,  1.59179688e-01, -2.35351562e-01, -1.25976562e-01,\n",
       "        2.68554688e-02, -5.29785156e-02, -6.59179688e-02, -2.17773438e-01,\n",
       "       -6.37817383e-03, -2.53906250e-01,  2.28515625e-01,  4.93164062e-02,\n",
       "        3.54003906e-02,  1.66992188e-01, -7.27539062e-02, -2.53906250e-01,\n",
       "       -1.34765625e-01,  3.69140625e-01,  1.83593750e-01, -1.64062500e-01,\n",
       "        2.26562500e-01, -8.88671875e-02,  3.69140625e-01,  5.54199219e-02,\n",
       "       -3.63769531e-02, -1.48437500e-01,  9.13085938e-02,  2.47955322e-04,\n",
       "        2.67578125e-01, -1.63085938e-01,  1.19628906e-01,  2.77343750e-01,\n",
       "       -1.49414062e-01,  1.33789062e-01, -8.25195312e-02, -1.74804688e-01,\n",
       "       -1.77734375e-01,  2.06054688e-01,  5.07812500e-02, -2.08007812e-01,\n",
       "       -1.74804688e-01,  9.66796875e-02,  6.98242188e-02, -5.79833984e-04,\n",
       "        9.22851562e-02,  7.95898438e-02,  1.41601562e-01,  8.72802734e-03,\n",
       "       -8.05664062e-02,  4.80957031e-02,  2.49023438e-01, -1.64062500e-01,\n",
       "       -4.66308594e-02, -2.81250000e-01, -1.66015625e-01, -2.22656250e-01,\n",
       "       -2.32421875e-01,  1.32812500e-01,  4.15039062e-02,  1.15234375e-01,\n",
       "       -7.66601562e-02, -1.10839844e-01, -1.97265625e-01,  3.06396484e-02,\n",
       "       -1.03515625e-01,  2.49023438e-02, -2.52685547e-02,  3.39355469e-02,\n",
       "        4.29687500e-02, -1.44531250e-01,  2.12402344e-02,  2.28271484e-02,\n",
       "       -1.88476562e-01,  3.22265625e-01, -1.13281250e-01, -7.61718750e-02,\n",
       "        2.94921875e-01, -1.33789062e-01, -1.80664062e-02, -6.25610352e-03,\n",
       "       -1.62353516e-02,  5.98144531e-02,  1.21582031e-01,  4.17480469e-02],\n",
       "      dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(embeddings_index['language'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Preprocess two tsv files here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adjust the ratio of the two classes of training data\n",
    "In training data, the ratio of good phrases to bad phrases is about one to thirty. That will make training classification unsatisfactory, so we need to adjust the ratio. Reducing bad phrases and adding good phrases are both common way.\n",
    "\n",
    "<font color=\"red\">**[ TODO ]**</font> Please adjust the ratio of good phrases to bad phrases in any way which you think is the best and output the number of two class for demo.\n",
    "\n",
    "You need to explain why you choose this ratio and how you do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### print the number of training data of two classes\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "train_0 = train.loc[train['class'] == 0]\n",
    "train_1 = train.loc[train['class'] == 1]\n",
    "\n",
    "train_0_len = len(train_0)\n",
    "train_1_len = len(train_1)\n",
    "\n",
    "desire_ratio = 1/4\n",
    "desire_n = int(train_1_len / desire_ratio)\n",
    "\n",
    "train_0_sample = train_0.sample(n=desire_n, random_state=42)\n",
    "\n",
    "train_shuffle = shuffle(pd.concat([train_1, train_0_sample]), random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### number words\n",
    "Let each word have its unique number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tok = Tokenizer(filters='')\n",
    "tok.fit_on_texts(pd.concat([train, test], ignore_index=True)['phrase'])\n",
    "vocab_size = len(tok.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### convert phrases into numbers\n",
    "Because model can't read words, so we have to do this transform. \n",
    "\n",
    "The number should be same as the last step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded_phrase = tok.texts_to_sequences(train['phrase'])\n",
    "test_encoded_phrase = tok.texts_to_sequences(test['phrase'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### padding\n",
    "Make all phrases become same length. The longest phrases in two tsv have five tokens. Hence, we should make the phrases whose lengths less than five become five by adding 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    6    1    3  481]\n",
      " [  11   27  125    2    1]\n",
      " [  28   27    1   22   15]\n",
      " [   0    1  260   95   23]\n",
      " [   1    7  610   19  143]\n",
      " [ 535 1125    4    1   92]\n",
      " [   0  482   88    1   29]\n",
      " [   0    1   10  120   12]\n",
      " [   0    0    2    1   33]\n",
      " [   0  381   34    1    3]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "max_ngram = 5\n",
    "x_train = pad_sequences(train_encoded_phrase, maxlen=max_ngram, padding='pre')\n",
    "x_test = pad_sequences(test_encoded_phrase, maxlen=max_ngram, padding='pre')\n",
    "print(x_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### one hot encodding label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "(30525, 2)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train=to_categorical(train['class'])\n",
    "y_test=to_categorical(test['class'])\n",
    "print(y_train[:5])\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split training data into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_val,y_train,y_val=train_test_split(x_train,y_train,test_size=0.20,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### creating the embedding matrix\n",
    "The embedding matrix is used by classification model. It should be a list of list. Each sub-list is an embedding vector of a word and the order of all embedding vectors should be same as *tokenizer*. It is stored in a dictionary. You can check it by `tok.word_index.items()`.\n",
    "\n",
    "<font color=\"red\">**[ TODO ]**</font> Make embedding matrix. If you don't need it for your classification model, you can skip it. We won't check it when demo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "vector_dimension = 300\n",
    "embedding_matrix = np.zeros((vocab_size, vector_dimension))\n",
    "\n",
    "for word, index in tok.word_index.items():\n",
    "    if index > vocab_size:\n",
    "        break\n",
    "    else:\n",
    "        try:\n",
    "            embedding_matrix[index] = embeddings_index[word]\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5596, 300)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build model\n",
    "<font color=\"red\">**[ TODO ]**</font> Please build your classification model by ***keras*** here. \n",
    "\n",
    "You **must** use the pre-trained word2vec model to represent the words of phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "764/764 [==============================] - 8s 9ms/step - loss: 0.3175 - accuracy: 0.8721\n",
      "Epoch 2/3\n",
      "764/764 [==============================] - 6s 8ms/step - loss: 0.1299 - accuracy: 0.9661\n",
      "Epoch 3/3\n",
      "764/764 [==============================] - 6s 8ms/step - loss: 0.0811 - accuracy: 0.9972\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 5, 300)            1678800   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 4)                 4880      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 10        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,683,690\n",
      "Trainable params: 1,683,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "        layers.Embedding(input_dim=embedding_matrix.shape[0],\n",
    "                         output_dim=300,\n",
    "                         input_length=5,\n",
    "                         weights=[embedding_matrix]),\n",
    "        tf.keras.layers.LSTM(4),\n",
    "        layers.Dense(units=2, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.binary_crossentropy,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=3)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train\n",
    "Train classification model here.\n",
    "\n",
    "<font color=\"red\">**[ TODO ]**</font> Adjust the hyperparameter to optimize the validation accuracy and validation loss.\n",
    "\n",
    "* The higher the accuracy, the better; the lower the validation, the better.\n",
    "* **number of epoch** and **batch size** are the most important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test\n",
    "\n",
    "<font color=\"red\">**[ TODO ]**</font> Test your model by test.tsv and output the accuracy. Your accuracy need to beat baseline: **0.97**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 745us/step - loss: 0.1563 - accuracy: 0.9895\n",
      "0.9894999861717224\n"
     ]
    }
   ],
   "source": [
    "accuracy = model.evaluate(x_test,y_test)\n",
    "print(accuracy[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show wrong prediction results\n",
    "Observing wrong prediction result may help you improve your prediction.\n",
    "\n",
    "<font color=\"red\">**[ TODO ]**</font> show the wrong prediction results like this: \n",
    "\n",
    "<img src=\"https://imgur.com/BOTMyZH.jpg\" width=30%><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_race(row):\n",
    "    if row['class'] == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase</th>\n",
       "      <th>class</th>\n",
       "      <th>labeled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>earn a reprimand from her</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>earn 10 points per dollar</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>earn instead of sharing it</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>earn money fast earn money</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>earn your confidence and respect</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>earn money from earn money</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>earn when everybody wants them</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>earn up to two more</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>earn a Division I scholarship</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>earn from home job money</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1201</th>\n",
       "      <td>earn money | earn money</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1227</th>\n",
       "      <td>earn money for introducing them</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1287</th>\n",
       "      <td>earn money online earn money</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1338</th>\n",
       "      <td>earn £ 10k more</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1452</th>\n",
       "      <td>earn the confidence and respect</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1470</th>\n",
       "      <td>earn , earned income</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1585</th>\n",
       "      <td>earn money , earn money</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1785</th>\n",
       "      <td>earn ThankYou Points ® that</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>earn the Most</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                phrase  class  labeled\n",
       "49           earn a reprimand from her      1        0\n",
       "88           earn 10 points per dollar      1        0\n",
       "270         earn instead of sharing it      1        0\n",
       "298         earn money fast earn money      1        0\n",
       "475   earn your confidence and respect      1        0\n",
       "541         earn money from earn money      1        0\n",
       "602     earn when everybody wants them      1        0\n",
       "765                earn up to two more      1        0\n",
       "801      earn a Division I scholarship      1        0\n",
       "960           earn from home job money      1        0\n",
       "1201           earn money | earn money      1        0\n",
       "1227   earn money for introducing them      1        0\n",
       "1287      earn money online earn money      1        0\n",
       "1338                   earn £ 10k more      1        0\n",
       "1452   earn the confidence and respect      1        0\n",
       "1470              earn , earned income      1        0\n",
       "1585           earn money , earn money      1        0\n",
       "1785       earn ThankYou Points ® that      1        0\n",
       "1795                     earn the Most      1        0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase</th>\n",
       "      <th>class</th>\n",
       "      <th>labeled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>earn your Masters Degree</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>% of them earn more</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>to earn $ 134 million</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>them earn a better living</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>make them earn their money</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>money earn money</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1512</th>\n",
       "      <td>earn Free Gear with Bonus</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1521</th>\n",
       "      <td>sites that earn a revenue</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1572</th>\n",
       "      <td>earn $ 1800</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1577</th>\n",
       "      <td>earn $ 275,000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          phrase  class  labeled\n",
       "197     earn your Masters Degree      0        1\n",
       "227          % of them earn more      0        1\n",
       "313        to earn $ 134 million      0        1\n",
       "590    them earn a better living      0        1\n",
       "615   make them earn their money      0        1\n",
       "1458            money earn money      0        1\n",
       "1512   earn Free Gear with Bonus      0        1\n",
       "1521   sites that earn a revenue      0        1\n",
       "1572                 earn $ 1800      0        1\n",
       "1577              earn $ 275,000      0        1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction = np.round(model.predict(x_test))\n",
    "\n",
    "wrong_index = np.equal(prediction, y_test)\n",
    "wrong_index = [not h[0] or not h[1] for h in wrong_index]\n",
    "\n",
    "wrong_predictions = test[wrong_index]\n",
    "\n",
    "se = (wrong_predictions.apply(lambda row: label_race(row), axis=1)).rename('labeled')\n",
    "\n",
    "wrong_predictions = pd.concat([wrong_predictions, se.to_frame()], axis=1)\n",
    "\n",
    "#display(wrong_predictions)\n",
    "display(wrong_predictions[wrong_predictions['class'] == 1])\n",
    "display(wrong_predictions[wrong_predictions['class'] == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TA's Notes\n",
    "\n",
    "If you complete the Assignment, please use [this link](https://docs.google.com/spreadsheets/d/1QGeYl5dsD9sFO9SYg4DIKk-xr-yGjRDOOLKZqCLDv2E/edit#gid=807282025) to reserve demo time.  \n",
    "The score is only given after TAs review your implementation, so <u>**make sure you make a appointment with a TA before you miss the deadline**</u> .  <br>After demo, please upload your assignment to eeclass. You just need to hand in this ipynb file and rename it as XXXXXXXXX(Your student ID).ipynb.\n",
    "<br>Note that **late submission will not be allowed**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Resource\n",
    "[Deep Learning with Python](https://tanthiamhuat.files.wordpress.com/2018/03/deeplearningwithpython.pdf)\n",
    "\n",
    "[Classification on IMDB](https://keras.io/examples/nlp/bidirectional_lstm_imdb/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
