{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJBKoaDrpWD-"
   },
   "source": [
    "# Week 06: Collocation Extraction\n",
    "In Assignment 5, we found all skip-grams and their frequencies in <u>*wiki1G.txt*</u>. This week, we want to use the result of assignment 5 to extract collocations of [AKL verbs](https://uclouvain.be/en/research-institutes/ilc/cecl/academic-keyword-list.html). We will use [Smadja’s algorithm](https://aclanthology.org/J93-1007.pdf) to do it. Here are some basic terms need to be explain. \n",
    "\n",
    "We take \"*dpend*\" as an example:\n",
    "\n",
    "<img src=\"https://imgur.com/cPyd7Gr.jpg\" >\n",
    "\n",
    "In this case, we want to find the collocations of \"depend\". Then, \"depend\" is called **base word** and marked as $W$. As for \"on\", \"the\", \"for\"..., they are called **collocate** and marked as $W_{i}$ where **i** represents their serial number. $P_{j}$ means the frequency of $W$ and $W_{i}$ with distance j. And **Freq** is the sum of frequencies of all distances.\n",
    "\n",
    "There are three conditions to filter the skipgram to find collocations. We will go through three conditions below.\n",
    "\n",
    "Considering that some students did not complete Assignment 5, in order to avoid them being unable to do assignment 6, we provide you with a file of calculated skipgram with frequencies, called **AKL_skipgram.tsv**. It only keeps the skipgrams with any AKL verb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TBxDYAFPpWEA"
   },
   "source": [
    "## Read Data\n",
    "<font color=\"red\">**[ TODO ]**</font> Please read <u>*AKL_skipgram.tsv*</u> and store it in the way you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import statistics\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "sVGZSm9fpWEB"
   },
   "outputs": [],
   "source": [
    "#### here are some hyperparameter\n",
    "k0 = 1\n",
    "k1 = 1\n",
    "U0 = 10\n",
    "base_word = \"depend\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ekEseC_PpWEB"
   },
   "outputs": [],
   "source": [
    "base_collocate = {}\n",
    "\n",
    "with open(os.path.join('data', 'AKL_skipgram.tsv')) as f:\n",
    "    \n",
    "    for line in f:\n",
    "        \n",
    "        tokens = line.split()\n",
    "        \n",
    "        if not tokens[0] in base_collocate:\n",
    "            collocate_freq = {}\n",
    "            collocate_freq[tokens[1]] = tokens[2:]\n",
    "            base_collocate[tokens[0]] = collocate_freq\n",
    "        else:\n",
    "            base_collocate[tokens[0]][tokens[1]] = tokens[2:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['69', '9', '14', '16', '10', '11', '0', '3', '2', '2', '2']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_collocate['depend']['all']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3N9bVOqOpWEB"
   },
   "source": [
    "## C1 Condition\n",
    "C1 helps eliminate the collocates that are not frequent enough. This condition specifies that the frequency of appearance of $W_{i}$ in the neighborhood of $W$ must be at least one standard deviation above the average.\n",
    "\n",
    "The formula is here:\n",
    "\n",
    "$$strength = \\frac{freq - \\bar{f}}{\\sigma} \\geq k_{0} = 1$$\n",
    "\n",
    "where $freq$ is the frequency of certain collocate, (e.g., 2573 for \"on\") and \n",
    "\n",
    "$\\bar{f}$ is the average frequencies of all collocates and \n",
    "\n",
    "${\\sigma}$ is the standard deviation of frequencies of all collocates.\n",
    "\n",
    "<font color=\"red\">**[ TODO ]**</font> Please follow the condition to filter the skipgrams of \"depend\" and keep some which pass the condition.\n",
    "\n",
    "The ouput sholud have `collocate` with its `strength`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "nQR2AYDxpWEC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a {strength:6.380}\n",
      "all {strength:1.150}\n",
      "also {strength:1.132}\n",
      "an {strength:1.367}\n",
      "and {strength:15.181}\n",
      "are {strength:1.962}\n",
      "as {strength:2.395}\n",
      "but {strength:1.529}\n",
      "by {strength:1.042}\n",
      "can {strength:1.421}\n",
      "do {strength:1.655}\n",
      "does {strength:5.298}\n",
      "for {strength:4.685}\n",
      "formula {strength:1.565}\n",
      "in {strength:5.875}\n",
      "is {strength:2.611}\n",
      "it {strength:2.287}\n",
      "its {strength:1.818}\n",
      "may {strength:2.864}\n",
      "not {strength:8.436}\n",
      "of {strength:23.459}\n",
      "on {strength:46.308}\n",
      "only {strength:1.295}\n",
      "or {strength:2.485}\n",
      "other {strength:1.655}\n",
      "properties {strength:1.042}\n",
      "s {strength:2.160}\n",
      "some {strength:1.187}\n",
      "such {strength:1.439}\n",
      "that {strength:7.246}\n",
      "the {strength:44.703}\n",
      "their {strength:2.828}\n",
      "these {strength:1.944}\n",
      "they {strength:2.233}\n",
      "this {strength:1.908}\n",
      "to {strength:8.418}\n",
      "type {strength:1.295}\n",
      "upon {strength:4.902}\n",
      "which {strength:4.379}\n",
      "will {strength:3.783}\n",
      "would {strength:1.601}\n"
     ]
    }
   ],
   "source": [
    "def C1_filter(base_word: str, base_collocate: dict):\n",
    "    \n",
    "    sigma = statistics.stdev([ int(f[0]) for f in (base_collocate[base_word][c] for c in base_collocate[base_word]) ])\n",
    "    avg = statistics.mean([ int(f[0]) for f in (base_collocate[base_word][c] for c in base_collocate[base_word]) ])\n",
    "    \n",
    "    filtered_collocate = []\n",
    "    \n",
    "    for c in base_collocate[base_word]:\n",
    "        \n",
    "        #根據公式實作\n",
    "        strength = ((float(base_collocate[base_word][c][0]) - avg) / sigma)\n",
    "        \n",
    "        if strength >= 1:\n",
    "            filtered_collocate.append([c, strength])\n",
    "            \n",
    "    return filtered_collocate\n",
    "\n",
    "filtered_by_C1 = C1_filter('depend', base_collocate)\n",
    "for l in filtered_by_C1:\n",
    "    print('{} {{strength:{:.3f}}}'.format(l[0], l[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYguZMfwpWED"
   },
   "source": [
    "<font color=\"green\">Expected output: </font> (The order isn't important.)\n",
    "\n",
    "> a {'strength': 6.381}   \n",
    "> all {'strength': 1.151}   \n",
    "> also {'strength': 1.133}   \n",
    "> an {'strength': 1.367}   \n",
    "> and {'strength': 15.183}   \n",
    "> are {'strength': 1.962}   \n",
    "> as {'strength': 2.395}   \n",
    "> but {'strength': 1.529}   \n",
    "> by {'strength': 1.042}   \n",
    "> can {'strength': 1.421}   \n",
    "> do {'strength': 1.656}   \n",
    "> does {'strength': 5.299}   \n",
    "> for {'strength': 4.686}   \n",
    "> formula {'strength': 1.565}   \n",
    "> in {'strength': 5.876}   \n",
    "> is {'strength': 2.611}   \n",
    "> it {'strength': 2.287}   \n",
    "> its {'strength': 1.818}   \n",
    "> may {'strength': 2.864}   \n",
    "> not {'strength': 8.437}   \n",
    "> of {'strength': 23.461}   \n",
    "> on {'strength': 46.313}   \n",
    "> only {'strength': 1.295}   \n",
    "> or {'strength': 2.485}   \n",
    "> other {'strength': 1.656}   \n",
    "> properties {'strength': 1.042}   \n",
    "> s {'strength': 2.161}   \n",
    "> some {'strength': 1.187}   \n",
    "> such {'strength': 1.439}   \n",
    "> that {'strength': 7.247}   \n",
    "> the {'strength': 44.707}   \n",
    "> their {'strength': 2.828}   \n",
    "> these {'strength': 1.944}   \n",
    "> they {'strength': 2.233}   \n",
    "> this {'strength': 1.908}   \n",
    "> to {'strength': 8.419}   \n",
    "> type {'strength': 1.295}   \n",
    "> upon {'strength': 4.902}   \n",
    "> which {'strength': 4.379}   \n",
    "> will {'strength': 3.784}   \n",
    "> would {'strength': 1.601}   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4lWRmQrGpWED"
   },
   "source": [
    "## C2 Condition\n",
    "C2 requires that the histogram of the 10 relative frequencies of appearance of $W_i$ within five words of $W$ (or $p^j_i$s) have at least one spike. If the histogram is flat, it will be rejected by this condition.\n",
    "\n",
    "The formula is here:\n",
    "\n",
    "$$spread = \\frac{\\Sigma^{10}_{j=1}(p^j_i - \\bar{p_i})^2}{10} \\geq U_{0} = 10$$\n",
    "\n",
    "where $p^j_i$ is the frequency of certain collocate with a distance of *j*, (e.g., 16 for \"on\" when its distance is -5) and \n",
    "\n",
    "$\\bar{p_i}$ is the average frequencies of \"on\" with any distance \n",
    "\n",
    "<font color=\"red\">**[ TODO ]**</font> Please follow C2 to filter the result of C1 and keep some which pass C2.\n",
    "\n",
    "The ouput sholud have `collocate` with `strength` and `spread`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "s2p0LbZtpWEE"
   },
   "outputs": [],
   "source": [
    "def C2_filter(base_word: str, filtered_by_C1: list, base_collocate: dict):\n",
    "    \n",
    "    collocate_spread = []\n",
    "    \n",
    "    #根據公式實作\n",
    "    for fc in filtered_by_C1:\n",
    "        avg = statistics.mean([int(f) for f in base_collocate[base_word][fc[0]][1:]])\n",
    "        spread = sum(pow((int(f) - avg), 2) for f in base_collocate[base_word][fc[0]][1:]) / 10\n",
    "        collocate_spread.append([fc[0], fc[1], spread])\n",
    "        \n",
    "    return collocate_spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "OJPDVL0WpWEE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a {strength:6.380, spread:777.29}\n",
      "all {strength:1.150, spread:29.89}\n",
      "also {strength:1.132, spread:208.96}\n",
      "an {strength:1.367, spread:56.29}\n",
      "and {strength:15.181, spread:2170.41}\n",
      "are {strength:1.962, spread:98.84}\n",
      "as {strength:2.395, spread:104.96}\n",
      "but {strength:1.529, spread:24.40}\n",
      "by {strength:1.042, spread:26.21}\n",
      "can {strength:1.421, spread:208.24}\n",
      "do {strength:1.655, spread:410.21}\n",
      "does {strength:5.298, spread:6477.09}\n",
      "for {strength:4.685, spread:376.65}\n",
      "formula {strength:1.565, spread:46.16}\n",
      "in {strength:5.875, spread:396.09}\n",
      "is {strength:2.611, spread:148.20}\n",
      "it {strength:2.287, spread:112.76}\n",
      "its {strength:1.818, spread:94.24}\n",
      "may {strength:2.864, spread:1352.24}\n",
      "not {strength:8.436, spread:12938.41}\n",
      "of {strength:23.459, spread:20132.64}\n",
      "on {strength:46.308, spread:420371.01}\n",
      "only {strength:1.295, spread:134.01}\n",
      "or {strength:2.485, spread:85.61}\n",
      "other {strength:1.655, spread:31.61}\n",
      "properties {strength:1.042, spread:30.21}\n",
      "s {strength:2.160, spread:125.85}\n",
      "some {strength:1.187, spread:15.29}\n",
      "such {strength:1.439, spread:27.45}\n",
      "that {strength:7.246, spread:1492.61}\n",
      "the {strength:44.703, spread:98586.04}\n",
      "their {strength:2.828, spread:209.56}\n",
      "these {strength:1.944, spread:180.01}\n",
      "they {strength:2.233, spread:316.09}\n",
      "this {strength:1.908, spread:71.09}\n",
      "to {strength:8.418, spread:3941.16}\n",
      "type {strength:1.295, spread:213.41}\n",
      "upon {strength:4.902, spread:4984.01}\n",
      "which {strength:4.379, spread:346.16}\n",
      "will {strength:3.783, spread:2250.05}\n",
      "would {strength:1.601, spread:412.44}\n"
     ]
    }
   ],
   "source": [
    "filtered_by_C2 = C2_filter(base_word, filtered_by_C1, base_collocate)\n",
    "for l in filtered_by_C2:\n",
    "    print('{} {{strength:{:.3f}, spread:{:.2f}}}'.format(l[0], l[1], l[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blC4RZwwpWEE"
   },
   "source": [
    "<font color=\"green\">Expected output: </font> (The order isn't important.)\n",
    "\n",
    "> a {'strength': 6.381, 'spread': 777.29}   \n",
    "> all {'strength': 1.151, 'spread': 29.89}   \n",
    "> also {'strength': 1.133, 'spread': 208.96}   \n",
    "> an {'strength': 1.367, 'spread': 56.29}   \n",
    "> and {'strength': 15.183, 'spread': 2170.41}   \n",
    "> are {'strength': 1.962, 'spread': 98.84}   \n",
    "> as {'strength': 2.395, 'spread': 104.96}   \n",
    "> but {'strength': 1.529, 'spread': 24.4}   \n",
    "> by {'strength': 1.042, 'spread': 26.21}   \n",
    "> can {'strength': 1.421, 'spread': 208.24}   \n",
    "> do {'strength': 1.656, 'spread': 410.21}   \n",
    "> does {'strength': 5.299, 'spread': 6477.09}   \n",
    "> for {'strength': 4.686, 'spread': 376.65}   \n",
    "> formula {'strength': 1.565, 'spread': 46.16}   \n",
    "> in {'strength': 5.876, 'spread': 396.09}   \n",
    "> is {'strength': 2.611, 'spread': 148.2}   \n",
    "> it {'strength': 2.287, 'spread': 112.76}   \n",
    "> its {'strength': 1.818, 'spread': 94.24}   \n",
    "> may {'strength': 2.864, 'spread': 1352.24}   \n",
    "> not {'strength': 8.437, 'spread': 12938.41}   \n",
    "> of {'strength': 23.461, 'spread': 20132.64}   \n",
    "> on {'strength': 46.313, 'spread': 420371.01}   \n",
    "> only {'strength': 1.295, 'spread': 134.01}   \n",
    "> or {'strength': 2.485, 'spread': 85.61}   \n",
    "> other {'strength': 1.656, 'spread': 31.61}   \n",
    "> properties {'strength': 1.042, 'spread': 30.21}   \n",
    "> s {'strength': 2.161, 'spread': 125.85}   \n",
    "> some {'strength': 1.187, 'spread': 15.29}   \n",
    "> such {'strength': 1.439, 'spread': 27.45}   \n",
    "> that {'strength': 7.247, 'spread': 1492.61}   \n",
    "> the {'strength': 44.707, 'spread': 98586.04}   \n",
    "> their {'strength': 2.828, 'spread': 209.56}   \n",
    "> these {'strength': 1.944, 'spread': 180.01}   \n",
    "> they {'strength': 2.233, 'spread': 316.09}   \n",
    "> this {'strength': 1.908, 'spread': 71.09}   \n",
    "> to {'strength': 8.419, 'spread': 3941.16}   \n",
    "> type {'strength': 1.295, 'spread': 213.41}   \n",
    "> upon {'strength': 4.902, 'spread': 4984.01}   \n",
    "> which {'strength': 4.379, 'spread': 346.16}   \n",
    "> will {'strength': 3.784, 'spread': 2250.05}   \n",
    "> would {'strength': 1.601, 'spread': 412.44}   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "naS9EpYZpWEF"
   },
   "source": [
    "## C3 Condition\n",
    "C3 keeps the interesting collocates by pulling out the peaks of the $p^j_i$ distributions.\n",
    "\n",
    "Formula:\n",
    "\n",
    "$$p^j_i \\geq \\bar{p_i} + (k_1 \\times \\sqrt{U_{i}})$$\n",
    "\n",
    "where $U_i$ is *spread* in C2 and\n",
    "\n",
    "$k_1$ is equal to 1 \n",
    "\n",
    "<font color=\"red\">**[ TODO ]**</font> Please follow the condition to filter the result of last step and keep some which pass C3.\n",
    "\n",
    "The ouput sholud have `base word, collocate, distance, strength, spread, peak, count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1aHJ7GHlpWEF"
   },
   "outputs": [],
   "source": [
    "def C3_filter(base_word, filtered_by_C2, base_collocate):\n",
    "    \n",
    "    collocate_peak = []\n",
    "    \n",
    "    for fc in filtered_by_C2:\n",
    "        \n",
    "        #根據公式實作\n",
    "        avg = statistics.mean([int(f) for f in base_collocate[base_word][fc[0]][1:]])\n",
    "        peak = avg + 1 * pow(fc[2], 0.5)\n",
    "        \n",
    "        for i in range(1, len(base_collocate[base_word][fc[0]])):\n",
    "            if int(base_collocate[base_word][fc[0]][i]) > peak:\n",
    "                \n",
    "                if i < 6:\n",
    "                    distance = i - 6\n",
    "                else:\n",
    "                    distance = i - 5\n",
    "                \n",
    "                collocate_peak.append([base_word,\n",
    "                                       fc[0],\n",
    "                                       distance,\n",
    "                                       fc[1],\n",
    "                                       fc[2],\n",
    "                                       peak,\n",
    "                                       base_collocate[base_word][fc[0]][i]])\n",
    "                \n",
    "    return collocate_peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "bFNox2TYpWEF",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(depend, a, 2) { strength:6.38, spread:777.29, peak:63.78, count:94 }\n",
      "(depend, all, -4) { strength:1.15, spread:29.89, peak:12.37, count:14 }\n",
      "(depend, all, -3) { strength:1.15, spread:29.89, peak:12.37, count:16 }\n",
      "(depend, also, -1) { strength:1.13, spread:208.96, peak:21.26, count:50 }\n",
      "(depend, an, 2) { strength:1.37, spread:56.29, peak:15.60, count:24 }\n",
      "(depend, an, 5) { strength:1.37, spread:56.29, peak:15.60, count:19 }\n",
      "(depend, and, 4) { strength:15.18, spread:2170.41, peak:131.29, count:149 }\n",
      "(depend, are, -5) { strength:1.96, spread:98.84, peak:21.34, count:27 }\n",
      "(depend, are, -4) { strength:1.96, spread:98.84, peak:21.34, count:22 }\n",
      "(depend, as, 4) { strength:2.39, spread:104.96, peak:24.04, count:30 }\n",
      "(depend, as, 5) { strength:2.39, spread:104.96, peak:24.04, count:28 }\n",
      "(depend, but, -2) { strength:1.53, spread:24.40, peak:13.94, count:14 }\n",
      "(depend, but, 5) { strength:1.53, spread:24.40, peak:13.94, count:15 }\n",
      "(depend, by, -5) { strength:1.04, spread:26.21, peak:11.42, count:13 }\n",
      "(depend, by, -4) { strength:1.04, spread:26.21, peak:11.42, count:12 }\n",
      "(depend, by, 4) { strength:1.04, spread:26.21, peak:11.42, count:13 }\n",
      "(depend, can, -1) { strength:1.42, spread:208.24, peak:22.83, count:49 }\n",
      "(depend, do, -2) { strength:1.66, spread:410.21, peak:29.95, count:70 }\n",
      "(depend, does, -2) { strength:5.30, spread:6477.09, peak:110.38, count:271 }\n",
      "(depend, for, 4) { strength:4.69, spread:376.65, peak:45.91, count:69 }\n",
      "(depend, formula, -4) { strength:1.57, spread:46.16, peak:15.99, count:19 }\n",
      "(depend, formula, 2) { strength:1.57, spread:46.16, peak:15.99, count:17 }\n",
      "(depend, formula, 5) { strength:1.57, spread:46.16, peak:15.99, count:19 }\n",
      "(depend, in, -5) { strength:5.88, spread:396.09, peak:53.00, count:55 }\n",
      "(depend, in, 4) { strength:5.88, spread:396.09, peak:53.00, count:62 }\n",
      "(depend, is, -5) { strength:2.61, spread:148.20, peak:27.17, count:37 }\n",
      "(depend, is, 5) { strength:2.61, spread:148.20, peak:27.17, count:29 }\n",
      "(depend, it, -3) { strength:2.29, spread:112.76, peak:23.82, count:39 }\n",
      "(depend, it, -2) { strength:2.29, spread:112.76, peak:23.82, count:24 }\n",
      "(depend, its, 2) { strength:1.82, spread:94.24, peak:20.31, count:36 }\n",
      "(depend, may, -1) { strength:2.86, spread:1352.24, peak:53.17, count:126 }\n",
      "(depend, not, -1) { strength:8.44, spread:12938.41, peak:161.05, count:388 }\n",
      "(depend, of, 4) { strength:23.46, spread:20132.64, peak:272.49, count:495 }\n",
      "(depend, on, 1) { strength:46.31, spread:420371.01, peak:905.66, count:2195 }\n",
      "(depend, only, 1) { strength:1.29, spread:134.01, peak:19.28, count:40 }\n",
      "(depend, or, 4) { strength:2.48, spread:85.61, peak:23.55, count:29 }\n",
      "(depend, or, 5) { strength:2.48, spread:85.61, peak:23.55, count:25 }\n",
      "(depend, other, 3) { strength:1.66, spread:31.61, peak:15.32, count:19 }\n",
      "(depend, other, 5) { strength:1.66, spread:31.61, peak:15.32, count:17 }\n",
      "(depend, properties, -4) { strength:1.04, spread:30.21, peak:11.80, count:12 }\n",
      "(depend, properties, -1) { strength:1.04, spread:30.21, peak:11.80, count:15 }\n",
      "(depend, properties, 3) { strength:1.04, spread:30.21, peak:11.80, count:15 }\n",
      "(depend, s, 4) { strength:2.16, spread:125.85, peak:23.72, count:41 }\n",
      "(depend, some, -3) { strength:1.19, spread:15.29, peak:11.01, count:13 }\n",
      "(depend, some, 2) { strength:1.19, spread:15.29, peak:11.01, count:14 }\n",
      "(depend, such, 4) { strength:1.44, spread:27.45, peak:13.74, count:17 }\n",
      "(depend, that, -3) { strength:7.25, spread:1492.61, peak:79.33, count:84 }\n",
      "(depend, that, -1) { strength:7.25, spread:1492.61, peak:79.33, count:132 }\n",
      "(depend, the, 2) { strength:44.70, spread:98586.04, peak:562.38, count:1140 }\n",
      "(depend, their, 2) { strength:2.83, spread:209.56, peak:30.68, count:52 }\n",
      "(depend, these, -2) { strength:1.94, spread:180.01, peak:24.72, count:48 }\n",
      "(depend, they, -1) { strength:2.23, spread:316.09, peak:30.68, count:63 }\n",
      "(depend, this, -4) { strength:1.91, spread:71.09, peak:19.53, count:28 }\n",
      "(depend, this, -2) { strength:1.91, spread:71.09, peak:19.53, count:22 }\n",
      "(depend, to, -1) { strength:8.42, spread:3941.16, peak:109.98, count:228 }\n",
      "(depend, type, 3) { strength:1.29, spread:213.41, peak:22.31, count:50 }\n",
      "(depend, upon, 1) { strength:4.90, spread:4984.01, peak:98.30, count:239 }\n",
      "(depend, which, -1) { strength:4.38, spread:346.16, peak:43.41, count:66 }\n",
      "(depend, will, -1) { strength:3.78, spread:2250.05, peak:68.93, count:159 }\n",
      "(depend, would, -1) { strength:1.60, spread:412.44, peak:29.71, count:70 }\n"
     ]
    }
   ],
   "source": [
    "filtered_by_C3 = C3_filter(base_word, filtered_by_C2, base_collocate)\n",
    "for l in filtered_by_C3:\n",
    "    print('({}, {}, {}) {{ strength:{:.2f}, spread:{:.2f}, peak:{:.2f}, count:{} }}'.format(l[0], l[1], l[2], l[3], l[4], l[5], l[6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9K43nWOqpWEF"
   },
   "source": [
    "<font color=\"green\">Expected output: </font> (The order isn't important.)\n",
    "\n",
    "> ('depend', 'a', 2) {'strength': 6.381, 'spread': 777.29, 'peak': 63.78, 'count': 94}   \n",
    "> ('depend', 'all', -4) {'strength': 1.151, 'spread': 29.89, 'peak': 12.367, 'count': 14}   \n",
    "> ('depend', 'all', -3) {'strength': 1.151, 'spread': 29.89, 'peak': 12.367, 'count': 16}   \n",
    "> ('depend', 'also', -1) {'strength': 1.133, 'spread': 208.96, 'peak': 21.255, 'count': 50}   \n",
    "> ('depend', 'an', 2) {'strength': 1.367, 'spread': 56.29, 'peak': 15.603, 'count': 24}   \n",
    "> ('depend', 'an', 5) {'strength': 1.367, 'spread': 56.29, 'peak': 15.603, 'count': 19}   \n",
    "> ('depend', 'and', 4) {'strength': 15.183, 'spread': 2170.41, 'peak': 131.288, 'count': 149}   \n",
    "> ('depend', 'are', -5) {'strength': 1.962, 'spread': 98.84, 'peak': 21.342, 'count': 27}   \n",
    "> ('depend', 'are', -4) {'strength': 1.962, 'spread': 98.84, 'peak': 21.342, 'count': 22}   \n",
    "> ('depend', 'as', 4) {'strength': 2.395, 'spread': 104.96, 'peak': 24.045, 'count': 30}   \n",
    "> ('depend', 'as', 5) {'strength': 2.395, 'spread': 104.96, 'peak': 24.045, 'count': 28}   \n",
    "> ('depend', 'but', -2) {'strength': 1.529, 'spread': 24.4, 'peak': 13.94, 'count': 14}   \n",
    "> ('depend', 'but', 5) {'strength': 1.529, 'spread': 24.4, 'peak': 13.94, 'count': 15}   \n",
    "> ('depend', 'by', -5) {'strength': 1.042, 'spread': 26.21, 'peak': 11.42, 'count': 13}   \n",
    "> ('depend', 'by', -4) {'strength': 1.042, 'spread': 26.21, 'peak': 11.42, 'count': 12}   \n",
    "> ('depend', 'by', 4) {'strength': 1.042, 'spread': 26.21, 'peak': 11.42, 'count': 13}   \n",
    "> ('depend', 'can', -1) {'strength': 1.421, 'spread': 208.24, 'peak': 22.831, 'count': 49}   \n",
    "> ('depend', 'do', -2) {'strength': 1.656, 'spread': 410.21, 'peak': 29.954, 'count': 70}   \n",
    "> ('depend', 'does', -2) {'strength': 5.299, 'spread': 6477.09, 'peak': 110.38, 'count': 271}   \n",
    "> ('depend', 'for', 4) {'strength': 4.686, 'spread': 376.65, 'peak': 45.907, 'count': 69}   \n",
    "> ('depend', 'formula', -4) {'strength': 1.565, 'spread': 46.16, 'peak': 15.994, 'count': 19}   \n",
    "> ('depend', 'formula', 2) {'strength': 1.565, 'spread': 46.16, 'peak': 15.994, 'count': 17}   \n",
    "> ('depend', 'formula', 5) {'strength': 1.565, 'spread': 46.16, 'peak': 15.994, 'count': 19}   \n",
    "> ('depend', 'in', -5) {'strength': 5.876, 'spread': 396.09, 'peak': 53.002, 'count': 55}   \n",
    "> ('depend', 'in', 4) {'strength': 5.876, 'spread': 396.09, 'peak': 53.002, 'count': 62}   \n",
    "> ('depend', 'is', -5) {'strength': 2.611, 'spread': 148.2, 'peak': 27.174, 'count': 37}   \n",
    "> ('depend', 'is', 5) {'strength': 2.611, 'spread': 148.2, 'peak': 27.174, 'count': 29}   \n",
    "> ('depend', 'it', -3) {'strength': 2.287, 'spread': 112.76, 'peak': 23.819, 'count': 39}   \n",
    "> ('depend', 'it', -2) {'strength': 2.287, 'spread': 112.76, 'peak': 23.819, 'count': 24}   \n",
    "> ('depend', 'its', 2) {'strength': 1.818, 'spread': 94.24, 'peak': 20.308, 'count': 36}   \n",
    "> ('depend', 'may', -1) {'strength': 2.864, 'spread': 1352.24, 'peak': 53.173, 'count': 126}   \n",
    "> ('depend', 'not', -1) {'strength': 8.437, 'spread': 12938.41, 'peak': 161.047, 'count': 388}   \n",
    "> ('depend', 'of', 4) {'strength': 23.461, 'spread': 20132.64, 'peak': 272.49, 'count': 495}   \n",
    "> ('depend', 'on', 1) {'strength': 46.313, 'spread': 420371.01, 'peak': 905.66, 'count': 2195}   \n",
    "> ('depend', 'only', 1) {'strength': 1.295, 'spread': 134.01, 'peak': 19.276, 'count': 40}   \n",
    "> ('depend', 'or', 4) {'strength': 2.485, 'spread': 85.61, 'peak': 23.553, 'count': 29}   \n",
    "> ('depend', 'or', 5) {'strength': 2.485, 'spread': 85.61, 'peak': 23.553, 'count': 25}   \n",
    "> ('depend', 'other', 3) {'strength': 1.656, 'spread': 31.61, 'peak': 15.322, 'count': 19}   \n",
    "> ('depend', 'other', 5) {'strength': 1.656, 'spread': 31.61, 'peak': 15.322, 'count': 17}   \n",
    "> ('depend', 'properties', -4) {'strength': 1.042, 'spread': 30.21, 'peak': 11.796, 'count': 12}   \n",
    "> ('depend', 'properties', -1) {'strength': 1.042, 'spread': 30.21, 'peak': 11.796, 'count': 15}   \n",
    "> ('depend', 'properties', 3) {'strength': 1.042, 'spread': 30.21, 'peak': 11.796, 'count': 15}   \n",
    "> ('depend', 's', 4) {'strength': 2.161, 'spread': 125.85, 'peak': 23.718, 'count': 41}   \n",
    "> ('depend', 'some', -3) {'strength': 1.187, 'spread': 15.29, 'peak': 11.01, 'count': 13}   \n",
    "> ('depend', 'some', 2) {'strength': 1.187, 'spread': 15.29, 'peak': 11.01, 'count': 14}   \n",
    "> ('depend', 'such', 4) {'strength': 1.439, 'spread': 27.45, 'peak': 13.739, 'count': 17}   \n",
    "> ('depend', 'that', -3) {'strength': 7.247, 'spread': 1492.61, 'peak': 79.334, 'count': 84}   \n",
    "> ('depend', 'that', -1) {'strength': 7.247, 'spread': 1492.61, 'peak': 79.334, 'count': 132}   \n",
    "> ('depend', 'the', 2) {'strength': 44.707, 'spread': 98586.04, 'peak': 562.384, 'count': 1140}   \n",
    "> ('depend', 'their', 2) {'strength': 2.828, 'spread': 209.56, 'peak': 30.676, 'count': 52}   \n",
    "> ('depend', 'these', -2) {'strength': 1.944, 'spread': 180.01, 'peak': 24.717, 'count': 48}   \n",
    "> ('depend', 'they', -1) {'strength': 2.233, 'spread': 316.09, 'peak': 30.679, 'count': 63}   \n",
    "> ('depend', 'this', -4) {'strength': 1.908, 'spread': 71.09, 'peak': 19.531, 'count': 28}   \n",
    "> ('depend', 'this', -2) {'strength': 1.908, 'spread': 71.09, 'peak': 19.531, 'count': 22}   \n",
    "> ('depend', 'to', -1) {'strength': 8.419, 'spread': 3941.16, 'peak': 109.979, 'count': 228}   \n",
    "> ('depend', 'type', 3) {'strength': 1.295, 'spread': 213.41, 'peak': 22.309, 'count': 50}   \n",
    "> ('depend', 'upon', 1) {'strength': 4.902, 'spread': 4984.01, 'peak': 98.298, 'count': 239}   \n",
    "> ('depend', 'which', -1) {'strength': 4.379, 'spread': 346.16, 'peak': 43.405, 'count': 66}   \n",
    "> ('depend', 'will', -1) {'strength': 3.784, 'spread': 2250.05, 'peak': 68.935, 'count': 159}   \n",
    "> ('depend', 'would', -1) {'strength': 1.601, 'spread': 412.44, 'peak': 29.709, 'count': 70}   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3bMqATOZpWEG"
   },
   "source": [
    "## Strongest Collocation\n",
    "There are too many collocations to check your result easily. Hence, we want you use the rules below to find out one strongest collocation for \"depend\".\n",
    "\n",
    "Rule:\n",
    "1. find the collocate with maximum **`strength`** value\n",
    "2. find the collocate with maximum **`count`** value\n",
    "\n",
    "If there're more than two collocations sharing same maximum `strength` value, please use rule 2 to find one as the answer. Otherwise, you can ignore Rule 2.\n",
    "\n",
    "<font color=\"red\">**[ TODO ]**</font> Please find out the strongest collocation for \"depend\" by the rules.\n",
    "\n",
    "The ouput format sholud be `(base word, collocate, distance)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "idhTENjepWEG"
   },
   "outputs": [],
   "source": [
    "def find_strongest_collocation(base_word, filtered_by_C3):\n",
    "    \n",
    "    max_index = 0\n",
    "    max_strength = filtered_by_C3[max_index][3]\n",
    "    max_count = filtered_by_C3[max_index][6]\n",
    "    \n",
    "    for i in range(0, len(filtered_by_C3)):\n",
    "        if filtered_by_C3[i][3] > max_strength:\n",
    "            max_index = i\n",
    "            max_strength = filtered_by_C3[i][3]\n",
    "            max_count = filtered_by_C3[i][6]\n",
    "        elif filtered_by_C3[i][3] == max_strength:\n",
    "            if filtered_by_C3[i][6] > max_count:\n",
    "                max_index = i\n",
    "                max_strength = filtered_by_C3[i][3]\n",
    "                max_count = filtered_by_C3[i][6]\n",
    "    \n",
    "    return [base_word, filtered_by_C3[max_index][1], filtered_by_C3[max_index][2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "dPxhbWqYpWEG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(depend, on, 1)\n"
     ]
    }
   ],
   "source": [
    "strongest_collocation = find_strongest_collocation(base_word, filtered_by_C3)\n",
    "print('({}, {}, {})'.format(strongest_collocation[0], strongest_collocation[1], strongest_collocation[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQFst4LLpWEG"
   },
   "source": [
    "<font color=\"green\">Expected output: </font>\n",
    "\n",
    "> ('depend', 'on', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ub8AdaebpWEG"
   },
   "source": [
    "## Find Helpful AKL Collocation\n",
    "Only one example cannot express how amazing what we just did, so here are some other AKL verbs selected for you to experience. \n",
    "\n",
    "<font color=\"red\">**[ TODO ]**</font> Please finish **combination** function to combine last four functions together and use it to find out strongest collocations for **AKL_verbs**. \n",
    "\n",
    "The ouput format sholud be `(base word, collocate, distance)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "fPZ6ulIJpWEG"
   },
   "outputs": [],
   "source": [
    "AKL_verbs = ['argue', 'can', 'consist', 'contrast', 'favour', 'lack', 'may', \n",
    "            'neglect', 'participate', 'present', 'rely', 'suggest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Yolxm1hfpWEG"
   },
   "outputs": [],
   "source": [
    "def combination(base_word: str, base_collocate: dict):\n",
    "    filtered_by_C1 = C1_filter(base_word, base_collocate)\n",
    "    filtered_by_C2 = C2_filter(base_word, filtered_by_C1, base_collocate)\n",
    "    filtered_by_C3 = C3_filter(base_word, filtered_by_C2, base_collocate)\n",
    "    strongest_collocation = find_strongest_collocation(base_word, filtered_by_C3)\n",
    "    print('({}, {}, {})'.format(strongest_collocation[0], strongest_collocation[1], strongest_collocation[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "31okIDBMpWEG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(argue, that, 1)\n",
      "(can, be, 1)\n",
      "(consist, of, 1)\n",
      "(contrast, in, -1)\n",
      "(favour, of, 1)\n",
      "(lack, of, 1)\n",
      "(may, be, 1)\n",
      "(neglect, of, 1)\n",
      "(participate, in, 1)\n",
      "(present, with, -3)\n",
      "(rely, on, 1)\n",
      "(suggest, that, 1)\n"
     ]
    }
   ],
   "source": [
    "for v in AKL_verbs:\n",
    "    combination(v, base_collocate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-laz0HgapWEH"
   },
   "source": [
    "<font color=\"green\">Expected output: </font>\n",
    "\n",
    "> ('argue', 'that', 1)   \n",
    "> ('can', 'be', 1)   \n",
    "> ('consist', 'of', 1)   \n",
    "> ('contrast', 'in', -1)   \n",
    "> ('favour', 'of', 1)   \n",
    "> ('lack', 'of', 1)   \n",
    "> ('may', 'be', 1)   \n",
    "> ('neglect', 'of', 1)   \n",
    "> ('participate', 'in', 1)   \n",
    "> ('present', 'with', -3)   \n",
    "> ('rely', 'on', 1)   \n",
    "> ('suggest', 'that', 1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XO64hc-OpWEH"
   },
   "source": [
    "## TA's Notes\n",
    "\n",
    "If you complete the Assignment, please use [this link](https://docs.google.com/spreadsheets/d/1QGeYl5dsD9sFO9SYg4DIKk-xr-yGjRDOOLKZqCLDv2E/edit#gid=206119035) to reserve demo time.  \n",
    "The score is only given after TAs review your implementation, so <u>**make sure you make a appointment with a TA before you miss the deadline**</u> .  <br>After demo, please upload your assignment to eeclass. You just need to hand in this ipynb file and rename it as XXXXXXXXX(Your student ID).ipynb.\n",
    "<br>Note that **late submission will not be allowed**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oayUKgdNpWEH"
   },
   "source": [
    "## Reference\n",
    "[Frank Smadja, Retrieving Collocations from Texts: Xtract, Computational Linguistics, Volume 19, 1993](https://aclanthology.org/J93-1007.pdf)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Assignment6.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
