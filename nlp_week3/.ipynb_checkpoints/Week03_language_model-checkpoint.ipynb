{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 03: Simple Language Model\n",
    "\n",
    "This week, we want you to create a Language Model (LM) to judge how common a sentence is.  \n",
    "More specificly, you need to <u>calculate the probability of a given setnence</u> showing in an article.  \n",
    "\n",
    "*No need to make your output exactly the same as ours. As long as it's *reasonable* (which means you can explain it), you get your points.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Language Model?\n",
    "\n",
    "<b>A statistical language model is a probability distribution over sequences of words. Given such a sequence, say of length $m$, it assigns a probability $P(w_1, \\ldots ,w_m)$ to the whole sequence.</b>\n",
    "<i>-- from <a href=\"https://en.wikipedia.org/wiki/Language_model\">Wikipedia</a></i>  \n",
    "\n",
    "To put it simply, Language Model calculates the probability of a word, a sequence of words, or a sentence.  \n",
    "This can be useful in many NLP tasks, like machine translation, spelling checking, speech recognition, etc.  \n",
    "\n",
    "Consider the following two sentence  \n",
    "\n",
    "> (1) He is looking <font color=\"green\">*to*</font> a new job.  \n",
    "\n",
    "and \n",
    "\n",
    "> (2) He is looking <font color=\"green\">*for*</font> a new job.  \n",
    "\n",
    "\n",
    "We can see that there's a grammar error in sentence 1, for which the LM should generate a lower probability.  \n",
    "Hence, if your LM is well-established, you are able to judge which sentence is more correct (or more common) among a set of candidate sentences.  \n",
    "\n",
    "But how to do so?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "### Step 1 - Ngram frequency\n",
    "\n",
    "First of all, we need to calculate 1- and 2-gram frequency from the corpus.  \n",
    "**Again**, yes. As we have said in the first class, word/phrase frequency plays an important role in NLP.\n",
    "\n",
    "You should be familiar with this procedure now, so let's skip the boring explanation.  \n",
    "<small>*Please refer to assignment 1 and 2 if you have any question.</small>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "with open(os.path.join('data', 'big.txt'), 'r') as f:\n",
    "    corpus = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg EBook of The Adventures of Sherlock Holmes\n",
      "by Sir Arthur Conan Doyle\n",
      "(#15 in our series by Sir Arthur Conan Doyle)\n",
      "\n",
      "Copyright laws are changing all over the world. Be sure to check the\n",
      "copyright laws for your country before downloading or redistributing\n",
      "this or any other Project Gutenberg eBook.\n",
      "\n",
      "This header should be the first thing seen when viewing this Project\n",
      "Gutenberg file.  Please do not remove it.  Do not change or edit the\n",
      "header without written permission.\n",
      "\n",
      "Please read the \"legal small print,\" and other information about the\n",
      "eBook and Project Gutenberg at the bottom of this file.  Included is\n",
      "important information about your specific rights and restrictions in\n",
      "how the file may be used.  You can also find out about how to make a\n",
      "donation to Project Gutenberg, and how to get involved.\n",
      "\n",
      "\n",
      "**Welcome To The World of Free Plain Vanilla Electronic Texts**\n",
      "\n",
      "**eBooks Readable By Both Humans and By Computers, Since 1971**\n",
      "\n",
      "*****These eBooks Were Prepared By Thousan\n"
     ]
    }
   ],
   "source": [
    "### Test block\n",
    "\n",
    "#print('{:.1000}'.format(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">**[ TODO ]**</font> Calculate 1- and 2-gram frequency with the given corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    @param text: a given string.\n",
    "    @return: a list tokenized text.\n",
    "    \"\"\"\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "def n_gram(tokens, n=2):\n",
    "    \"\"\"\n",
    "    @param tokens: tokenized text.\n",
    "    @param n: length of n_gram.\n",
    "    @return: a list of n_gram.\n",
    "    \"\"\"\n",
    "    n_gram = []\n",
    "    for i in range(0, len(tokens)-1):\n",
    "        n_gram.append(\" \".join([tokens[i], tokens[i+1]]))\n",
    "    \n",
    "    return n_gram\n",
    "\n",
    "def unigram_freq(text):\n",
    "    \"\"\" \n",
    "    @param text: a given string.\n",
    "    @return: a dict of tokens frequency.\n",
    "    \"\"\"\n",
    "    \n",
    "    unigram_frequency = {}\n",
    "    \n",
    "    tokens = tokenize(text)\n",
    "    for t in tokens:\n",
    "        if t in unigram_frequency:\n",
    "            unigram_frequency[t] += 1\n",
    "        else:\n",
    "            unigram_frequency[t] = 1\n",
    "            \n",
    "    return unigram_frequency\n",
    "\n",
    "def bigram_freq(text):\n",
    "    \"\"\" \n",
    "    @param text: a given string.\n",
    "    @return: a dict of bigram frequency.\n",
    "    \"\"\"\n",
    "    \n",
    "    bigram_frequency = {}\n",
    "    \n",
    "    bigram = n_gram(tokenize(text), 2)\n",
    "    \n",
    "    for bi in bigram:\n",
    "        if bi in bigram_frequency:\n",
    "            bigram_frequency[bi] += 1\n",
    "        else:\n",
    "            bigram_frequency[bi] = 1\n",
    "            \n",
    "    return bigram_frequency\n",
    "\n",
    "\n",
    "uni_freq = unigram_freq(corpus)\n",
    "bi_freq = bigram_freq(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test block\n",
    "\n",
    "#print(tokenize(corpus)[:1000])\n",
    "\n",
    "#print(bi_freq[\"how are\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Probability of a word\n",
    "\n",
    "#### Unigram  \n",
    "Probability of a word, or a *unigram*, is simple: its occurence devided by the total word count should do the trick.  \n",
    "$$P(w) = \\frac{Freq(w)}{N}$$  \n",
    ", where $N = $ count of all words.  \n",
    "But the unigram language model doesn't consider other context words, so we choose to use bigram model here.  \n",
    "\n",
    "#### Bigram  \n",
    "As to 2-gram probability, we would use the Maximum Likelihood Estimate (MLE) to calculate it.  \n",
    "\n",
    "$$P_{mle}(w_i|w_{i-1}) = \\frac{Freq(w_{i-1}, w_i)}{Freq(w_{i-1})}$$\n",
    "\n",
    "For example, if we have a corpus with \n",
    "\n",
    "> can you tell me about any good cantonese restaurants  \n",
    "> tell me about chez panisse  \n",
    "> can you give me a listing of the kinds of food that are available  \n",
    "\n",
    "Then the probability of *tell* preceding *me* is $P_{mle}(me|tell) = \\frac{2}{2} = 1$.  \n",
    "Similarly, the probability of *me* preceding *about* is $P_{mle}(about|me) = \\frac{2}{3}$.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">**[ TODO ]**</font> Calculate the bigram probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P_mle(word: str, pre_word: str, unigram_freq: dict, bigram_freq: dict):\n",
    "    \"\"\"\n",
    "    @param word, pre_word: the words to be calculated.\n",
    "    @param unigram_freq and bigram_freq: Frequency of tokens.\n",
    "    @return: a float of the probability of pre_word preceding word.\n",
    "    \"\"\"\n",
    "    \n",
    "    whole_word = ' '.join([pre_word, word])\n",
    "    \n",
    "    if whole_word not in bigram_freq or pre_word not in unigram_freq:\n",
    "        return 0.0\n",
    "    \n",
    "    return bigram_freq[whole_word] / unigram_freq[pre_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1: 0.0; \n",
      "p2: 0.004672897196261682\n"
     ]
    }
   ],
   "source": [
    "p1 = P_mle('book', 'read', uni_freq, bi_freq)\n",
    "p2 = P_mle('books', 'read', uni_freq, bi_freq)\n",
    "print(f'p1: {p1}; \\np2: {p2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">Expected output: </font> `p1` should be **smaller** than `p2` (expetced to be `0`).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1: 0.0; \n",
      "p2: 0.0010061808250682765\n"
     ]
    }
   ],
   "source": [
    "p1 = P_mle('books', 'a', uni_freq, bi_freq)\n",
    "p2 = P_mle('book', 'a', uni_freq, bi_freq)\n",
    "print(f'p1: {p1}; \\np2: {p2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">Expected output: </font> `p1` should be **smaller** than `p2` (expetced to be `0`).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1: 0.0006567066163191594; \n",
      "p2: 0.015514693810540141\n"
     ]
    }
   ],
   "source": [
    "p1 = P_mle('have', 'he', uni_freq, bi_freq)\n",
    "p2 = P_mle('has', 'he', uni_freq, bi_freq)\n",
    "print(f'p1: {p1}; \\np2: {p2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">Expected output: </font> `p1` should be **smaller** than `p2`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Sentence probability\n",
    "\n",
    "Now we have the probability of every word and bigram. But how should we combine them and get the probability of a sentence?  \n",
    "\n",
    "Recall the chain rule in probability: \n",
    "$$P(X_1, X_2, X_3, X_4) = P(X_1) \\cdot P(X_2|X_1) \\cdot P(X_3|X_1,X_2) \\cdot P(X_4|X_1, X_2, X_3)$$  \n",
    "And with Markove Assumption, we know that \n",
    "$$P(w_1, w_2, \\dots, w_n) \\approx P(w_2|w_1) \\cdot P(w_3|w_2) \\cdot \\dots \\cdot P(w_n|w_{n-1})$$\n",
    "\n",
    "For example,\n",
    "$$P(I\\:want\\:a\\:cake) \\approx P(want|I) \\cdot P(a|want) \\cdot P(cake|a)$$\n",
    "\n",
    "Since we have already built a method to calculate $P(w_{i}|w_{i-1})$, we can get the probability of a sentence through this equation.  \n",
    "Note that it's recommended to sum them under $log$-scale to prevent underflow, because gram probabilities are mostly some small floating points, \n",
    "$$log(p_1 \\cdot p_2 \\dots p_n) = log(p_1) + log(p_2) + \\dots + log(p_n) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">**[ TODO ]**</font> Calculate the sentence probability. (Please do so under the `log` scale to prevent underflow)  \n",
    "<small>*Hint: `math`</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sentence_prob(sentence: str, P_model, unigram_freq: dict, bigram_freq: dict):\n",
    "    \"\"\" Calculate the probability of a given sentence with given P-model.\n",
    "    @param sentence: The given sentence\n",
    "    @param P: The probability model to use.\n",
    "    @param unigram_freq and bigram_freq: Frequncies of tokens.\n",
    "    @return: A float indicating the probability.\n",
    "    \"\"\"\n",
    "    \n",
    "    log_probability = 0.0\n",
    "    \n",
    "    words = tokenize(sentence)\n",
    "    \n",
    "    for i in range(0, len(words)-1):\n",
    "        p_now = P_model(words[i+1], words[i], unigram_freq, bigram_freq)\n",
    "        log_probability += math.log(p_now)\n",
    "    \n",
    "    return log_probability\n",
    "\n",
    "# Note: Log(0) is UNDEFINED."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1: -28.481591605450063; \n",
      "p2: -25.99877356688323\n"
     ]
    }
   ],
   "source": [
    "p1 = sentence_prob('He have to take the medicine.', P_mle, uni_freq, bi_freq)\n",
    "p2 = sentence_prob('He has to take the medicine.', P_mle, uni_freq, bi_freq)\n",
    "print(f'p1: {p1}; \\np2: {p2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">Expected output: </font> `p1` should be **smaller** than `p2`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looks good so far?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this model heavily depends on how large your dataset is.   \n",
    "If you give it the word or gram not existing in the corpus, the probability of it will be $0$, which causes problems during the calculation (because you can't divide numbers by $0$).  \n",
    "\n",
    "Try this! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "math domain error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-100-faa4f51194fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msentence_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"He has to eat medicine.\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mP_mle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muni_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbi_freq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-97-513585b46d20>\u001b[0m in \u001b[0;36msentence_prob\u001b[1;34m(sentence, P_model, unigram_freq, bigram_freq)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mp_now\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mP_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munigram_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbigram_freq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mlog_probability\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp_now\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mlog_probability\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: math domain error"
     ]
    }
   ],
   "source": [
    "sentence_prob(\"He has to eat medicine.\", P_mle, uni_freq, bi_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See? Now you get the problem.  \n",
    "Surely you can expand your dataset to decrease the number of unseen phrases, but it's impossible to cover all phrases in the world.  \n",
    "Here the smoothing technique comes to rescue!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - smoothing\n",
    "\n",
    "A naÃ¯ve solution is to set the default frequency as `1`.  \n",
    "For example, if the original frequency is\n",
    "\n",
    "> hello: 2  \n",
    "> world: 1  \n",
    "> empty: 0  \n",
    "> null: 0  \n",
    "\n",
    ", after defaulting the frequency to 1, it would be\n",
    "\n",
    "> hello: 3  \n",
    "> world: 2  \n",
    "> empty: <font color=\"red\">**1**</font><br/>\n",
    "> null: <font color=\"red\">**1**</font><br/>\n",
    "\n",
    "\n",
    "And if the frequency is all added by 1, the probability of bigram will now be  \n",
    "$$ P_{add1} = \\frac{Freq(w_{i-1}, w_i)+1}{Freq(w_{i-1})+N}$$\n",
    ", where $N = count\\:of\\:all\\:tokens$ .  \n",
    "\n",
    "This is called the **Add-1 Estimation**, or also **Laplace Smoothing**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">**[ TODO ]**</font> Implement the Laplace smoothing.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P_add1(word: str, pre_word: str, unigram_freq: dict, bigram_freq: dict):\n",
    "    \"\"\" [TODO] Return the P(word|pre_word) with Laplace smoothing. \"\"\"\n",
    "    \n",
    "    whole_word = ' '.join([pre_word, word])\n",
    "    \n",
    "    whole_word_frequency = 1\n",
    "    pre_word_frequency = 1\n",
    "    \n",
    "    if whole_word in bigram_freq:\n",
    "        whole_word_frequency = bigram_freq[whole_word] + 1\n",
    "      \n",
    "    if pre_word in unigram_freq:\n",
    "        pre_word_frequency = unigram_freq[pre_word] + len(bigram_freq)\n",
    "\n",
    "        \n",
    "    return  whole_word_frequency / pre_word_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1: 2.5267264490144504e-06; \n",
      "p2: 5.046337998672813e-06\n"
     ]
    }
   ],
   "source": [
    "p1 = P_add1('medicine', 'eat', uni_freq, bi_freq)\n",
    "p2 = P_add1('medicine', 'take', uni_freq, bi_freq)\n",
    "print(f'p1: {p1}; \\np2: {p2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">Expected output:</font> `p1` shouldn't throw any error, and `p1` should be **smaller** than `p2`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, Add-1 estimation is a really naive method and doesn't always produce good results.  \n",
    "There are better smooting techiniques like <a href=\"https://en.wikipedia.org/wiki/Good%E2%80%93Turing_frequency_estimation\">Good-Turing Estimation</a> or <a href=\"https://en.wikipedia.org/wiki/Kneser%E2%80%93Ney_smoothing\">Kneser-Ney Smoothing</a>. Feel free to give it a try!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test your LM\n",
    "\n",
    "Seems that you've done a great job! Let's give it some tests to see if it works as expected.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He have to take the medicine. -47.14066475513946\n",
      "He has to eat the medicine. -51.062554498018464\n",
      "He has to take the medicine. -45.517670524818996\n"
     ]
    }
   ],
   "source": [
    "test_cases = [\n",
    "  'He have to take the medicine.',\n",
    "  'He has to eat the medicine.',\n",
    "  'He has to take the medicine.'\n",
    "]\n",
    "\n",
    "for sent in test_cases:\n",
    "    print(sent, sentence_prob(sent, P_add1, uni_freq, bi_freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">Expected output:</font> The last sentence should return the highest probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He is looking to a job. -48.09599330267389\n",
      "He is looking for a job. -46.95945376093201\n"
     ]
    }
   ],
   "source": [
    "test_cases = [\n",
    "  'He is looking to a job.',\n",
    "  'He is looking for a job.'\n",
    "]\n",
    "\n",
    "for sent in test_cases:\n",
    "  print(sent,  sentence_prob(sent, P_add1, uni_freq, bi_freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">Expected output:</font> The second sentence should return a higher probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Think about it\n",
    "Think about the following questions. **TA will ask your answers during the demo.**  \n",
    "\n",
    "1. What would happen if you use MLE Estimation to compare two sentences with different lengthes?  \n",
    "   Examples. (1) *He school nice and happy.* and (2) *He went to school yesterday and had a nice time there.*  \n",
    "\n",
    "2. According to Q1, Do you think MLE model is fair? If so, why? If not, how could you improve this? (Just think about it; no need to actually implement it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He has to take the medicine. -45.517670524818996\n",
      "He is looking for a job. -46.95945376093201\n"
     ]
    }
   ],
   "source": [
    "test_cases = [\n",
    "  'He has to take the medicine.',\n",
    "  'He is looking for a job.'\n",
    "]\n",
    "\n",
    "for sent in test_cases:\n",
    "  print(sent,  sentence_prob(sent, P_add1, uni_freq, bi_freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulation!** You've successfully built your own language model.  \n",
    "You should have caught a glimpse of LM's power despite its plainness.  \n",
    "Feel free to play around with your LM and improve it, like  \n",
    "1. using a larger dataset, \n",
    "2. applying better smoothing technique, or \n",
    "3. considering longer gram dependency (3- to 5-gram are resonable length).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TA's note\n",
    "\n",
    "\n",
    "Remember to <b><a href=\"https://docs.google.com/spreadsheets/d/1QGeYl5dsD9sFO9SYg4DIKk-xr-yGjRDOOLKZqCLDv2E/edit?usp=sharing\">make an appoiment with TA</a> to demo/explain your implementation <u>before <font color=\"red\">10/7 15:30</font></u></b> .  \n",
    "You should also save your file as {student_id}.ipynb and submit it to <a href=\"https://eeclass.nthu.edu.tw/course/homework/2384\">eeclass</a> .  \n",
    "**Late submission is not allowed.**  "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0e56d558c252a4887cb9bc110bf182e2aa9946109639baab21a0ce4014d1d01c"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
