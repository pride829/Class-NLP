{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "infectious-buddy",
   "metadata": {},
   "source": [
    "# Week 14: Senquence Classification with BERT\n",
    "\n",
    "The assignment this week is to do the senquence classification. This may sound like what we had done in the previous assignment, but we are using BERT as our classifier this week, rather than Machine Learning.\n",
    "\n",
    "The objective is to judge the CEFR level of a sentence.  \n",
    "[CEFR](https://www.cambridgeenglish.org/exams-and-tests/cefr/) is a standard for describing language ability of a person. It consists of 6 levels, A1, A2, B1, B2, C1, and C2, going from easier to harder.  \n",
    "A dataset that contains sentences with the corresponding CEFR level is provided, and you have to use BERT and train a sentence classifier with this dataset.  \n",
    "The dataset is collected and processed from a research by Alison Chi, æŽæ›¸å‰, æŽå† éœ– and Prof. Chang. Thank you all for allowing us to use it in the lecture.\n",
    "\n",
    "As to the implementatin, we will introduce you the [ðŸ¤— transformers](https://huggingface.co/) library, which is mantained by huggingface company, as the training framework this week. [Pytorch](https://pytorch.org/) is used as the deep learning backend in this tutorial, but with the transformers library, all codes can be easily changed to tensorflow if you prefer so.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "given-speaker",
   "metadata": {},
   "source": [
    "## Prepare your environment\n",
    "\n",
    "Again, we highly recommend you to install all packages with a virtual environment manager, like [venv](https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/) or [conda](https://docs.conda.io/projects/conda/en/latest/user-guide/getting-started.html), to prevent version conflicts of different packages.  \n",
    "\n",
    "If you haven't used it before and don't know which to use, I would suggest you start with [mamba](https://github.com/mamba-org/mamba#installation) or [mambaforge](https://github.com/conda-forge/miniforge#mambaforge)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b9c26f",
   "metadata": {},
   "source": [
    "### Install CUDA\n",
    "\n",
    "Deep learning is a computionally extensive process. It takes lots of time if relying only on the CPU, especially when it's trained on a large dataset. That's why using GPU instead is generally recommended.  \n",
    "To use GPU for computation, you have to install [CUDA toolkit](https://developer.nvidia.com/cuda-toolkit) as well as the [cuDNN library](https://developer.nvidia.com/cudnn) provided by NVIDIA.  \n",
    "\n",
    "If you already had CUDA installed on your machine, then great! You're done here.  \n",
    "If you don't, you can refer to [Appendix 1](#Appendix-1-Install-CUDA) to see how to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08c0036",
   "metadata": {},
   "source": [
    "### Install python packages\n",
    "\n",
    "Dependencies:\n",
    "\n",
    "1. `numpy`: for matrix operation\n",
    "2. `scikit-learn`: for label encoding\n",
    "3. `datasets`: for data preparation\n",
    "4. `transformers`: for model loading and finetuing\n",
    "5. (choose one) `tensorflow` / `pytorch`: the backend DL framework\n",
    "   - Note that the tf/pt version must support the CUDA version you've installed if you want to use GPU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e18cc24",
   "metadata": {},
   "source": [
    "### Select GPU(s) for your backend\n",
    "\n",
    "Skip this section if you have no intension of using GPU with tensorflow/pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2c2f1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# select your GPU. Note that this should be set before you load tensorflow or pytorch.\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "# To use multiple GPUs, combine all GPU ID with commas\n",
    "# e.g. >>> os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,3'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19aa1f9c",
   "metadata": {},
   "source": [
    "#### >> Check Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "254c8721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# Check if any GPU is used\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d69ce74",
   "metadata": {},
   "source": [
    "#### >> Check Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e144684",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pride\\miniconda3\\envs\\week14\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\pride\\miniconda3\\envs\\week14\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\pride\\miniconda3\\envs\\week14\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\pride\\miniconda3\\envs\\week14\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\pride\\miniconda3\\envs\\week14\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\pride\\miniconda3\\envs\\week14\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\pride\\miniconda3\\envs\\week14\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\pride\\miniconda3\\envs\\week14\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\pride\\miniconda3\\envs\\week14\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\pride\\miniconda3\\envs\\week14\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\pride\\miniconda3\\envs\\week14\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\pride\\miniconda3\\envs\\week14\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow._api.v1.config' has no attribute 'list_physical_devices'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15488/2722427907.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Check if your GPU(s) is(are) listed below\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist_physical_devices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\miniconda3\\envs\\week14\\lib\\site-packages\\tensorflow\\python\\util\\deprecation_wrapper.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_dw_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Accessing local variables before they are created.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m     \u001b[0mattr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dw_wrapped_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m     if (self._dw_warning_count < _PER_MODULE_WARNING_LIMIT and\n\u001b[0;32m    108\u001b[0m         name not in self._dw_deprecated_printed):\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow._api.v1.config' has no attribute 'list_physical_devices'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Check if your GPU(s) is(are) listed below \n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small-radio",
   "metadata": {},
   "source": [
    "## Prepare the dataset\n",
    "\n",
    "Before starting the training, of course we need to load and process our dataset - but wait a sec. Let's decide which model we want to use first.  \n",
    "\n",
    "In case you are not familiar with it, [BERT](https://arxiv.org/abs/1810.04805) (**B**idirectional **E**ncoder **R**epresentations from **T**ransformers) is a language model proposed by Google AI in 2018, and it's one of the most popular models used in NLP area.  \n",
    "However, we will not directly use BERT in this tutorial, because it's large and needs plenty of time to train. Instead, we are using [DistilBert](https://medium.com/huggingface/distilbert-8cf3380435b5) this week.  \n",
    "\n",
    "DistilBERT is a distilled (è’¸é¤¾) version of BERT that is much more light-weighted than original model while reserving 95% of its original accuracy, which makes it perfect for our task today.  \n",
    "\n",
    "Further Reading:\n",
    " - [BERT Explained: A Complete Guide with Theory and Tutorial](https://towardsml.com/2019/09/17/bert-explained-a-complete-guide-with-theory-and-tutorial/) by Samia, 2019.\n",
    " - [é€²æ“Šçš„ BERTï¼šNLP ç•Œçš„å·¨äººä¹‹åŠ›èˆ‡é·ç§»å­¸ç¿’](https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html) by æŽå­Ÿ, 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "manufactured-disney",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model you want to use. Available models can be found here: https://huggingface.co/models\n",
    "MODEL_NAME = 'distilbert-base-uncased'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-woman",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "Similar to `transformers` library, `datasets` is also a package provided by huggingface. It contains many public datasets online and can help us with the data processing.  \n",
    "We can use `load_dataset` function to read the input `.csv` file.\n",
    "\n",
    "Reference:\n",
    " - [Official datasets document](https://huggingface.co/docs/datasets)\n",
    " - [datasets.load_dataset](https://huggingface.co/docs/datasets/loading.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "personalized-tourist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "minute-tension",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-e7064344f20d7991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/pride829/.cache/huggingface/datasets/csv/default-e7064344f20d7991/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66cb63b215e749dd9172736dd228e134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cc48c4cbbe34aa5a1088c92b8d3200d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/pride829/.cache/huggingface/datasets/csv/default-e7064344f20d7991/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6967adf01d614feeadd97428b099fcf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('csv', data_files = os.path.join('data', 'evp.train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ccd2ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'level'],\n",
      "    num_rows: 20720\n",
      "})\n",
      "{'text': 'You can contact me by e-mail.', 'level': 'A1'}\n",
      "['My mother is having her car repaired.', 'You can contact me by e-mail.', 'He had a break for the weekend, and he called me: \"I am in London, so, if you want to see me, it\\'s the time!\"', \"Research shows that 40 percent of the program's viewers are aged over 55.\", \"I'd guess she's about my age.\"]\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the data structure\n",
    "print(dataset['train'])\n",
    "print(dataset['train'][1])\n",
    "print(dataset['train']['text'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabc16be",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Same as before, texts should be tokenized, embedded, and padded before put into the model.  \n",
    "But don't worry, with the libraries from huggingface, the procedure is much easier now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1302e662",
   "metadata": {},
   "source": [
    "#### Sentence processing\n",
    "\n",
    "Different pre-trained language models may have their own preprocessing models, and that's why we should use the tokenizers trained along with that model. In our case, we are using distilBERT, so we should use the distilBERT tokenizer.  \n",
    "\n",
    "With huggingface, loading different tokenizer is extremely easy: just import the AutoTokenizer from `transformers` and tell it what model you plan to use, and it will handle everything for you.\n",
    "\n",
    "Reference:\n",
    " - [transformers.AutoTokenizer](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "831e6af0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "264d0572b709482599d3102264e33407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc02a214cecd4777bd9a8fb184f1c67e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f11cfd7277d14810b9f15aa73dc26d10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dc9048dc5ca45858aa7359e871b4969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer # For tokenization\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6323c2f3",
   "metadata": {},
   "source": [
    "#### Play with BERTTokenizer\n",
    "\n",
    "<small><i>*You can safely skip this section if you're already familar with BERTTokenizer.</i></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8b908a",
   "metadata": {},
   "source": [
    "Let's play with this tokenizer a little bit before we go on.\n",
    "\n",
    "Using this tokenizer is pretty easy: just call this object, and it processes the sentences for you.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f39baab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2023, 2061, 1011, 2170, 1000, 3819, 3944, 1000, 2001, 2061, 15640, 1010, 2004, 2092, 2004, 12532, 4648, 4726, 2149, 2013, 2746, 2000, 2115, 4418, 3004, 2153, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = \"This so-called \\\"Perfect Evening\\\" was so disappointing, as well as discouraging us from coming to your Circle Theatre again.\"\n",
    "\n",
    "embeddings = tokenizer(example)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5e43a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_base.BatchEncoding"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4040ddd8",
   "metadata": {},
   "source": [
    "As you can see, the sentence has already been tokenized and embedded. A default attention mask is returned as well.  \n",
    "\n",
    "To get the token back is easy as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "033ecb19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] this so - called \" perfect evening \" was so disappointing , as well as disco ##ura ##ging us from coming to your circle theatre again . [SEP]\n"
     ]
    }
   ],
   "source": [
    "decoded_tokens = tokenizer.batch_decode(embeddings['input_ids'])\n",
    "print(' '.join(decoded_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f093abd",
   "metadata": {},
   "source": [
    "You may notice that there're some weird stuffs appearing in your task, like `[CLS]` or `[SEP]`. The word *discouraging* is even split into `disco` `##ura` and `##ging` .  \n",
    "`[CLS]`, `[SEP]`, `[UKN]` and `[MASK]` are four symbols introduced by BERT, which stand for \"classification\", \"seperator\", \"unknown\" and \"mask\" respectively.  \n",
    "As to `##` thing, it's called a *wordpiece*, which is a concept [also brought out by Google](https://arxiv.org/abs/1609.08144). The key idea is to split words into common sub-word units, so the number of rare words can significantly decrease.\n",
    "\n",
    "Besides simply tokenizing a sentence, there are also many parameters you can set. You can play with it a bit, changing the parameters and observe the difference.\n",
    "\n",
    "Document:\n",
    " - [transformers.Tokenizer](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1118f333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2023,  2061,  1011,  2170,  1000,  3819,  3944,  1000,  2001,\n",
       "          2061, 15640,  1010,  2004,  2092,  2004, 12532,  4648,  4726,  2149,\n",
       "          2013,  2746,  2000,  2115,  4418,  3004,  2153,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EXAMPLE: directly transform into embedding tensor\n",
    "embeddings = tokenizer(example,\n",
    "                       # padding='longest',         # padding strategy\n",
    "                       # max_length=10,             # how long to pad sentences\n",
    "                       is_split_into_words=False,\n",
    "                       truncation=True,\n",
    "                       return_tensors='pt',         # 'tf' for tensofrlow, 'pt' for pytorch, 'np' for numpy\n",
    "                       # return_length=True         # whether to return length\n",
    "                       # Any other parameters you want to try\n",
    "                      )\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ae1c931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_base.BatchEncoding"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e667000",
   "metadata": {},
   "source": [
    "#### Label processing\n",
    "\n",
    "Before we start to process sentences in the whole dataset, don't forget we need to process labels as well.\n",
    "\n",
    "In the following section, I will introduce you the OneHotEncoder provided by scikit-learn.\n",
    "\n",
    "Documents:\n",
    " - [sklearn.preprocessing.OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "135b2f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# First, declare a new encoder\n",
    "encoder = OneHotEncoder(sparse = False)\n",
    "# Then, let the encoder learns all features in the given dataset\n",
    "# Keep in mind that all `fit` functions in sklearn only make the encoder learn from the data, not transforming the data yet.\n",
    "encoder = encoder.fit(np.reshape(dataset['train']['level'], (-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b71cc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "LABEL_COUNT = len(encoder.categories_[0])\n",
    "print(LABEL_COUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068f7e29",
   "metadata": {},
   "source": [
    "#### Play with OneHotEncoder\n",
    "\n",
    "<small><i>*You can safely skip this section if you're already familar with sklearn.</i></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b30ebc",
   "metadata": {},
   "source": [
    "One thing you should always keep in mind is: features learned by OneHotEncoder are always treated as arrays, because it allows multi-field features. (See its [document](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder) for an example)  \n",
    "That's why you have to reshape the level into (-1, 1), i.e. from `['A1', 'B1', 'C1', ...]` to `[['A1'], ['B1'], ['C1'], ...]` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4a89d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['A1', 'A2', 'B1', 'B2', 'C1', 'C2'], dtype='<U2')]\n"
     ]
    }
   ],
   "source": [
    "# Let's see what features has the encoder captured\n",
    "print(encoder.categories_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63ee709d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# use `encoder.transform` to get the one-hot code of a label\n",
    "print(encoder.transform([['B1'], ['C2']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9501dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['B1']]\n"
     ]
    }
   ],
   "source": [
    "# To decode, use `encoder.inverse_transform` instead\n",
    "print(encoder.inverse_transform([[0, 0, 1, 0, 0, 0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72153561",
   "metadata": {},
   "source": [
    "#### [ TODO ] Process the data\n",
    "\n",
    "With the tokenizor and encoder prepared, we can write a function to process our dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5669f4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataslice):\n",
    "    \"\"\" Input: a batch of your dataset\n",
    "        Example: { 'text': [['sentence1'], ['setence2'], ...],\n",
    "                   'label': ['label1', 'label2', ...] }\n",
    "    \"\"\"\n",
    "    \n",
    "    embeddings = tokenizer(dataslice['text'],\n",
    "                           #padding=True,         # padding strategy\n",
    "                           # max_length=10,             # how long to pad sentences\n",
    "                           #is_split_into_words=False,\n",
    "                           #truncation=True,\n",
    "                           #return_tensors='pt',         # 'tf' for tensofrlow, 'pt' for pytorch, 'np' for numpy\n",
    "                           # return_length=True         # whether to return length\n",
    "                           # Any other parameters you want to try\n",
    "                          )\n",
    "\n",
    "    encoder = OneHotEncoder(sparse = False)\n",
    "    encoder = encoder.fit(np.reshape(dataslice['level'], (-1, 1)))\n",
    "    \n",
    "    \n",
    "    label = encoder.transform(np.reshape(dataslice['level'], (-1, 1)))\n",
    "\n",
    "    #print(embeddings)\n",
    "    #print(type(label))\n",
    "    \n",
    "    output = {}\n",
    "    \n",
    "    #output['input_ids'] = embeddings['input_ids'].numpy()\n",
    "    #output['attention_mask'] = embeddings['attention_mask'].numpy()\n",
    "    \n",
    "    output['input_ids'] = embeddings['input_ids']\n",
    "    output['attention_mask'] = embeddings['attention_mask']\n",
    "    output['label'] = label\n",
    "    \n",
    "    return output\n",
    "    \n",
    "    \"\"\" Output: a batch of processed dataset\n",
    "        Example: { 'input_ids': ...,\n",
    "                   'attention_masks': ...,\n",
    "                   'label': ... }\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc612333",
   "metadata": {},
   "source": [
    "Now, map the function to the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "caa298f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0e85dde337d46979875efad344f59a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processed_data = dataset.map(preprocess,    # your processing function\n",
    "                             batched = True # Process in batches so it can be faster\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74953918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['attention_mask', 'input_ids', 'label', 'level', 'text'],\n",
      "        num_rows: 20720\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'input_ids': [101, 2026, 2388, 2003, 2383, 2014, 2482, 13671, 1012, 102],\n",
       " 'label': [0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " 'level': 'B1',\n",
       " 'text': 'My mother is having her car repaired.'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at processed dataset\n",
    "print(processed_data)\n",
    "processed_data['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f670f7c6",
   "metadata": {},
   "source": [
    "### DataCollator\n",
    "\n",
    "You may notice that we didn't pad the sentences in the preprocessing function, because we are going to do it during the training time.  \n",
    "\n",
    "To do the training-time processing, we can use the DataCollator Class provided by `transformers`. What's even better is, transformers already provides a class that handles padding for us!\n",
    "\n",
    " - [transformers.DataCollatorWithPadding](https://huggingface.co/docs/transformers/master/en/main_classes/data_collator#transformers.DataCollatorWithPadding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1add1e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# declare a collator to do padding during traning.\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "888c20a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataCollatorWithPadding(tokenizer=PreTrainedTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}), padding=True, max_length=None, pad_to_multiple_of=None, return_tensors='pt')\n"
     ]
    }
   ],
   "source": [
    "print(data_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be824b94",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57154197",
   "metadata": {},
   "source": [
    "### Preparation\n",
    "\n",
    "We can load the pretrained model from `transformers`.  \n",
    "Generally, you need to build your own model on top of BERT if you want to use BERT for some downstream tasks, but again, sequence classification is a popular topic. With the support from `transformers` library, all works can be done in two lines of codes: \n",
    "\n",
    "1. Load `AutoModelForSequenceClassification` Class.\n",
    "2. Load the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a09afeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "740d2883b6334f5dbaf79fac661cfff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/256M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Change to TFAutoModelForSequenceClassification if you're using tensoflow\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased',\n",
    "                                                           num_labels = LABEL_COUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d085bc",
   "metadata": {},
   "source": [
    "#### [ TODO ] Split train/val data\n",
    "\n",
    "The `Dataset` class we prepared before already has the `train_test_split` method. You can use it to split your dataset.\n",
    "\n",
    "Document:\n",
    " - [datasets.Dataset - Sort, shuffle, select, split, and shard](https://huggingface.co/docs/datasets/process.html#sort-shuffle-select-split-and-shard)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9352c946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ TODO ] Choose the validation data size\n",
    "# [ DONE ]\n",
    "train_val_dataset = processed_data['train'].train_test_split(test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "545bc915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['attention_mask', 'input_ids', 'label', 'level', 'text'],\n",
      "        num_rows: 16576\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['attention_mask', 'input_ids', 'label', 'level', 'text'],\n",
      "        num_rows: 4144\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Take a look at split data\n",
    "print(train_val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0667bc9",
   "metadata": {},
   "source": [
    "#### [ TODO ] Setup training parameters\n",
    "\n",
    "We are using the TrainerAPI to do the training. Trainer is yet another utility provided by huggingface, which helps you train the model with ease.  \n",
    "\n",
    "Document:\n",
    "- [transformers.TrainingArguments](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.TrainingArguments)\n",
    "- [transformers.Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "189a7144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to TFTrainingArguments, TFTrainer if you're using tensoflow\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8bf81339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ TODO ] Set and tune your training properties\n",
    "LEARNING_RATE = 5e-05\n",
    "BATCH_SIZE = 8\n",
    "EPOCH = 3\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = 'model',\n",
    "    learning_rate = LEARNING_RATE,\n",
    "    per_device_train_batch_size = BATCH_SIZE,\n",
    "    per_device_eval_batch_size = BATCH_SIZE,\n",
    "    num_train_epochs = EPOCH,\n",
    "    # You can also set other parameters here\n",
    ")\n",
    "\n",
    "# Now give all information to a trainer.\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_val_dataset[\"train\"],\n",
    "    eval_dataset = train_val_dataset[\"test\"],\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = data_collator,\n",
    "    # You can also set other parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abd2b6d",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Training is pretty easy. Simply ask the trainer to train the model for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bae54732",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: level, text.\n",
      "***** Running training *****\n",
      "  Num examples = 16576\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6216\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6216' max='6216' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6216/6216 44:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.395000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.358700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.346800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.343300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.274400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.264900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.261200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.254400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.195400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.168000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.166400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.152500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to model/checkpoint-500\n",
      "Configuration saved in model/checkpoint-500/config.json\n",
      "Model weights saved in model/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to model/checkpoint-1000\n",
      "Configuration saved in model/checkpoint-1000/config.json\n",
      "Model weights saved in model/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to model/checkpoint-1500\n",
      "Configuration saved in model/checkpoint-1500/config.json\n",
      "Model weights saved in model/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to model/checkpoint-2000\n",
      "Configuration saved in model/checkpoint-2000/config.json\n",
      "Model weights saved in model/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to model/checkpoint-2500\n",
      "Configuration saved in model/checkpoint-2500/config.json\n",
      "Model weights saved in model/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-2500/special_tokens_map.json\n",
      "Saving model checkpoint to model/checkpoint-3000\n",
      "Configuration saved in model/checkpoint-3000/config.json\n",
      "Model weights saved in model/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-3000/special_tokens_map.json\n",
      "Saving model checkpoint to model/checkpoint-3500\n",
      "Configuration saved in model/checkpoint-3500/config.json\n",
      "Model weights saved in model/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-3500/special_tokens_map.json\n",
      "Saving model checkpoint to model/checkpoint-4000\n",
      "Configuration saved in model/checkpoint-4000/config.json\n",
      "Model weights saved in model/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-4000/special_tokens_map.json\n",
      "Saving model checkpoint to model/checkpoint-4500\n",
      "Configuration saved in model/checkpoint-4500/config.json\n",
      "Model weights saved in model/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-4500/special_tokens_map.json\n",
      "Saving model checkpoint to model/checkpoint-5000\n",
      "Configuration saved in model/checkpoint-5000/config.json\n",
      "Model weights saved in model/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-5000/special_tokens_map.json\n",
      "Saving model checkpoint to model/checkpoint-5500\n",
      "Configuration saved in model/checkpoint-5500/config.json\n",
      "Model weights saved in model/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-5500/special_tokens_map.json\n",
      "Saving model checkpoint to model/checkpoint-6000\n",
      "Configuration saved in model/checkpoint-6000/config.json\n",
      "Model weights saved in model/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-6000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6216, training_loss=0.26146100325320526, metrics={'train_runtime': 2692.1203, 'train_samples_per_second': 18.472, 'train_steps_per_second': 2.309, 'total_flos': 446386949619552.0, 'train_loss': 0.26146100325320526, 'epoch': 3.0})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e63e5b3",
   "metadata": {},
   "source": [
    "You can see that Trainer saves some ckeckpoints, so you can load your model from those checkpoints if you want to fallback to a specific version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1501549",
   "metadata": {},
   "source": [
    "### Save for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d400687f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in model/finetuned/config.json\n",
      "Model weights saved in model/finetuned/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(os.path.join('model', 'finetuned'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af492d0",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "We've known how to train a model now, but how to really use it for predicting results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5587893d",
   "metadata": {},
   "source": [
    "### Load finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "71dc129d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model/finetuned/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"model/finetuned\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"multi_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model/finetuned/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at model/finetuned.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Same, change to TFxxxxxx if you are using tensorflow\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "mymodel = AutoModelForSequenceClassification.from_pretrained(os.path.join('model', 'finetuned'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1583078f",
   "metadata": {},
   "source": [
    "### Get the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872d987e",
   "metadata": {},
   "source": [
    "Given six example sentences..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5623b71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    # A2\n",
    "    \"Remember to write me a letter.\",\n",
    "    # B2\n",
    "    \"Strawberries and cream - a perfect combination.\",\n",
    "    \"This so-called \\\"Perfect Evening\\\" was so disappointing, as well as discouraging us from coming to your Circle Theatre again.\",\n",
    "    # C1\n",
    "    \"Some may altogether give up their studies, which I think is a disastrous move.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc10a6dd",
   "metadata": {},
   "source": [
    "...all you need to do is to transform them to embeddings, and then you can get predictions by calling your finetuned model.  \n",
    "\n",
    "Note that, since you don't have a DataCollator to pad the sentence and do the matrix transformation for you, you have to pad and transform the matrice on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e2276924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9050, -2.1400, -4.5247, -5.2337, -4.6768, -4.7797],\n",
       "        [-7.5280, -6.9617, -5.1950, -1.7884,  1.2411, -2.9181],\n",
       "        [-7.5949, -6.8332, -5.4501,  0.4709, -0.3539, -4.5285],\n",
       "        [-7.6892, -7.3095, -5.6460,  0.0680, -0.4457, -3.2494]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform the sentences into embeddings\n",
    "input = tokenizer(examples, truncation=True, padding=True, return_tensors=\"pt\") # change return_tensors if youre using tensorflow\n",
    "# Get the output\n",
    "logits = mymodel(**input).logits\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e1e99d",
   "metadata": {},
   "source": [
    "Logits aren't very readable for us. Let's use softmax activation to transform them into more probability-like numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a2ea86b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.7795e-01, 1.7124e-02, 1.5774e-03, 7.7628e-04, 1.3548e-03, 1.2224e-03],\n",
       "        [1.4584e-04, 2.5692e-04, 1.5034e-03, 4.5344e-02, 9.3810e-01, 1.4653e-02],\n",
       "        [2.1681e-04, 4.6438e-04, 1.8516e-03, 6.9025e-01, 3.0256e-01, 4.6537e-03],\n",
       "        [2.6095e-04, 3.8147e-04, 2.0134e-03, 6.1017e-01, 3.6506e-01, 2.2118e-02]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or `from tensorflow import nn` and `nn.softmax`\n",
    "from torch import nn\n",
    "\n",
    "predicts = nn.functional.softmax(logits, dim = -1)\n",
    "predicts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a08934",
   "metadata": {},
   "source": [
    "#### [ TODO ] transform logits back to labels\n",
    "\n",
    "Now you've got the output. Write a function to map it back into labels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f6fba1d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A1', 'C1', 'B2', 'B2']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [ TODO ] try to process the result\n",
    "# [ DONE ]\n",
    "def predicts_to_labels(predicts):\n",
    "    label_cat = ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']\n",
    "    _, indices = torch.max(predicts, 1)\n",
    "    \n",
    "    labels = []\n",
    "    for idx in indices:\n",
    "        labels.append(label_cat[idx])\n",
    "    \n",
    "    return labels\n",
    "\n",
    "predicts_to_labels(predicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ec453e",
   "metadata": {},
   "source": [
    "## [ TODO ] Evaluation\n",
    "\n",
    "It's your turn!  \n",
    "Load the testing data and calculate your accuracy.\n",
    "\n",
    "We want you to calculate two kinds of accuracy, exact accuracy and fuzzy accuracy, which will be explained in the following section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f9c92380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-6b7e41f952df3365\n",
      "Reusing dataset csv (/home/pride829/.cache/huggingface/datasets/csv/default-6b7e41f952df3365/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b168021bd72741a89aa271ba1e9fcc0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/pride829/.cache/huggingface/datasets/csv/default-6b7e41f952df3365/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-26675242a5204b1c.arrow\n"
     ]
    }
   ],
   "source": [
    "# [ TODO ] \n",
    "# load test data\n",
    "\n",
    "test_dataset = load_dataset('csv', data_files = os.path.join('data', 'evp.test.csv'))\n",
    "\n",
    "# preprocess\n",
    "\n",
    "processed_test_data = test_dataset.map(preprocess,    # your processing function\n",
    "                             batched = True # Process in batches so it can be faster\n",
    "                            )\n",
    "\n",
    "# get predictions\n",
    "\n",
    "test_input = tokenizer(processed_test_data['train']['text'], truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# transform predictions back into labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "26b6c3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = mymodel(**test_input).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8e862280",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = nn.functional.softmax(logits, dim = -1)\n",
    "predicts\n",
    "test_predicts = predicts_to_labels(predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "424b4cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'A1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'A1',\n",
       " 'C1',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'A1',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'A1',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'A1',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'A1',\n",
       " 'A2',\n",
       " 'A1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'A2',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'A1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'A1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'A1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'A2',\n",
       " 'A1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'A1',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'A1',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'A1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'A1',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'A1',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'A1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'A1',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'A1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'A1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'A1',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'A1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'A1',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'A2',\n",
       " 'A1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'A1',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'A1',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'A1',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'A1',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'A1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'A1',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'A1',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'A1',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'A1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'A1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'A2',\n",
       " 'A1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " ...]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "76f0fd67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'A1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'A1',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'A1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'A1',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'A2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'A2',\n",
       " 'A1',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'A1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'A1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'A2',\n",
       " 'A1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'A1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'A1',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'A1',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'A1',\n",
       " 'A2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'A1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'A1',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'A1',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'A1',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'A2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'A1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'A1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'A1',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'A1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'A2',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'A1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'A1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'A1',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'A1',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'A1',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'A1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'A1',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'A1',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'A1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'A1',\n",
       " 'C1',\n",
       " 'A1',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'A1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'A1',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'A1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'A1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'A1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'A1',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'A1',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'A1',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'A1',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'A1',\n",
       " 'A1',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " ...]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_test_data['train']['level']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6ea8f8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C2: C2\n",
      "B2: B2\n",
      "B2: B2\n",
      "C2: C2\n",
      "C1: C1\n",
      "A1: A2\n",
      "B1: B1\n",
      "B2: B2\n",
      "A2: A2\n",
      "C2: C1\n"
     ]
    }
   ],
   "source": [
    "# we still recommend you to print out some predictions to check if the outputs are resonable and if you need to adjust your model at the end of every step.\n",
    "\n",
    "for idx, (sent, level) in enumerate(zip(processed_test_data['train']['level'], test_predicts)):\n",
    "    if idx >= 10: break\n",
    "    print(f'{level}: {sent}') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cef45a",
   "metadata": {},
   "source": [
    "### Six Level Accuracy\n",
    "\n",
    "Exact accuracy is what you've been familiar with:\n",
    "\n",
    "$\n",
    "accuracy = \\frac{\\#exactly\\:the\\:same\\:levels}{\\#total}\n",
    "$\n",
    "\n",
    "Example:\n",
    "```\n",
    "Prediction:   A1 A2 B1 B2 C1 C2\n",
    "Ground truth: A2 B1 B1 B2 B2 C2\n",
    "                    ^  ^     ^\n",
    "```\n",
    "\n",
    "The six level accuracy is $\\frac{3}{6} = 0.5$\n",
    "\n",
    "As the requirement, <u>your exact accuracy should be higher than $0.5$</u>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6545d339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5430434782608695"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "same_count = 0\n",
    "\n",
    "for idx, (sent, level) in enumerate(zip(processed_test_data['train']['level'], test_predicts)):\n",
    "    if(sent == level):\n",
    "        same_count += 1\n",
    "        \n",
    "same_count / len(test_predicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaa2778",
   "metadata": {},
   "source": [
    "### [ TODO ] Three Level Accuracy\n",
    "\n",
    "Three Level Accuracy is used when you only want the general of right or wrong.\n",
    "\n",
    "$\n",
    "accuracy = \\frac{\\#the\\:same\\:ABC\\:levels}{\\#total}\n",
    "$\n",
    "\n",
    "Example:\n",
    "```\n",
    "Prediction:   A1 A2 B1 B2 C1 C2\n",
    "Ground truth: A2 B1 B1 B2 B2 C2\n",
    "              ^     ^  ^     ^\n",
    "```\n",
    "\n",
    "The six level accuracy is $\\frac{4}{6} = 0.667$\n",
    "\n",
    "As the requirement, <u>your exact accuracy should be higher than $0.6$</u>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "75bc6232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "same_count = 0\n",
    "\n",
    "for idx, (sent, level) in enumerate(zip(processed_test_data['train']['level'], test_predicts)):\n",
    "    if(sent[0] == level[0]):\n",
    "        same_count += 1\n",
    "        \n",
    "same_count / len(test_predicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae91c91",
   "metadata": {},
   "source": [
    "### [ TODO ] Fuzzy accuracy\n",
    "\n",
    "However, the level of a sentence is relatively subjective. Generally speaking, $\\pm1$ errors are allowed in the real evaluation in linguistic area.  \n",
    "\n",
    "For example, if the label is actually 'B1', but the model predicts 'B2', we still consider the prediction good enough, and this also applys when the model predicts 'A2'.\n",
    "\n",
    "Hence, the fuzzy accuracy is\n",
    "\n",
    "$\n",
    "accuracy = \\frac{\\#good\\:enough\\:answers}{\\#total}\n",
    "$\n",
    "\n",
    "Example:\n",
    "```\n",
    "Prediction:   0 1 2 3 4 5\n",
    "Ground truth: 0 1 1 3 3 3\n",
    "              ^ ^ ^ ^ ^\n",
    "```\n",
    "\n",
    "The fuzzy accuracy is $\\frac{5}{6} = 0.833$\n",
    "\n",
    "As the requirement, <u>your accuracy should be higher than $0.8$</u>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a5b853c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8704347826086957"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_cat = ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']\n",
    "\n",
    "label_dict = {}\n",
    "idx = 0\n",
    "for l in label_cat:\n",
    "    label_dict[l] = idx\n",
    "    idx += 1\n",
    "\n",
    "same_count = 0\n",
    "    \n",
    "for idx, (sent, level) in enumerate(zip(processed_test_data['train']['level'], test_predicts)):\n",
    "    if(abs(label_dict[sent] - label_dict[level]) <= 1):\n",
    "        same_count += 1\n",
    "\n",
    "same_count / len(test_predicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b404b1bf",
   "metadata": {},
   "source": [
    "## TA's note\n",
    "\n",
    "Congratuation! You've finished the assignment this week.  \n",
    "Don't forget to <b>[make an appoiment with TA](https://docs.google.com/spreadsheets/d/1QGeYl5dsD9sFO9SYg4DIKk-xr-yGjRDOOLKZqCLDv2E/edit#gid=134737606) to demo/explain your implementation <u>before <font color=\"red\">12/23 15:30</font></u></b> .  \n",
    "Also make sure you submit your `{student_id}.ipynb` to [eeclass](https://eeclass.nthu.edu.tw/course/homework/6053).\n",
    "\n",
    "This is the last assignment of this class. A TA will still be at the online classroom and answer your question during the class time in the following weeks, and you can have make-up demos at that time.  \n",
    "Prof. Chang's office hours are in Tues. to Thurs. evenings. You can come to Delta 712 to consult him at that time, but make sure you follow the appointment rules written on the bulletin or [the appointment sheet](https://docs.google.com/spreadsheets/d/1QGeYl5dsD9sFO9SYg4DIKk-xr-yGjRDOOLKZqCLDv2E/edit?usp=sharing).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ab7eb0",
   "metadata": {},
   "source": [
    "## Appendix \n",
    "\n",
    "<a name=\"Appendix-1-Install-CUDA\"></a>\n",
    "\n",
    "### Appendix 1 - Install CUDA\n",
    "\n",
    "1. Check your GPU vs. CUDA compatibility:\n",
    "   - [NVIDIA -> Your GPU Compute Capability](https://developer.nvidia.com/cuda-gpus) -> GeForce and TITAN Products\n",
    "2. Check library vs. CUDA compatibility: \n",
    "   - Pytorch: [Previous PyTorch Versions](https://pytorch.org/get-started/previous-versions/)\n",
    "   - Tensorflow: [Linux/MacOX](https://www.tensorflow.org/install/source#tested_build_configurations) or [Windows](https://www.tensorflow.org/install/source_windows#tested_build_configurations)\n",
    "3. Note the highest CUDA version that fits your system.\n",
    "\n",
    "#### >> for conda/mamba users\n",
    "\n",
    "You can directly install CUDA library with the selected CUDA version.\n",
    "1. Get [the driver for NVIDIA GPU](https://www.nvidia.com/download/index.aspx)\n",
    "2. `conda/mamba install -c conda-forge cudatoolkit=${VERSION}`\n",
    "\n",
    "#### >> for non-conda users\n",
    "\n",
    "1. Get [the driver for NVIDIA GPU](https://www.nvidia.com/download/index.aspx)\n",
    "2. Download and install [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit-archive)\n",
    "3. Download and install [cuDNN Library](https://developer.nvidia.com/rdp/cudnn-archive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb0dd1d",
   "metadata": {},
   "source": [
    "<a name=\"Appendix-2-TAs-Environmental-setup\"></a>\n",
    "\n",
    "### Appendix 2 - TA's Environmental Setup\n",
    "\n",
    "The following is my setup for this notebook. You can refer to it if you encounter some environmental issues.  \n",
    "\n",
    "System: Ubuntu 18.04.6, x64, With GPU support. All packages are installed in new conda environment with channels default to conda-forge.\n",
    "\n",
    "1. Python 3.8.12\n",
    "2. numpy=1.21.2\n",
    "3. cudatoolkit=11.1.74\n",
    "4. pytorch=1.8.2\n",
    "5. datasets=1.16.1\n",
    "6. transformers=4.12.5\n",
    "7. scikit-learn=1.0.1\n",
    "\n",
    "Notes:\n",
    "\n",
    " - conda create -n week14 python=3.8 & conda activate week14\n",
    " - conda config --add channels conda-forge\n",
    " - conda config --set channel_priority strict\n",
    " - conda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch-lts -c nvidia\n",
    " - conda install transformers\n",
    " - conda install datasets scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf0f256",
   "metadata": {},
   "source": [
    "### Appendix 3 - Further Readings\n",
    "\n",
    "1. [Huggingface Official Tutorials](https://github.com/huggingface/notebooks/tree/master/examples)\n",
    "2. How to use Bert with other downstream tasks: [How to use BERT from the Hugging Face transformer library](https://towardsdatascience.com/how-to-use-bert-from-the-hugging-face-transformer-library-d373a22b0209): \n",
    "3. Training with pytorch backend: [transformers-tutorials](https://github.com/abhimishra91/transformers-tutorials)\n",
    "4. A more complicated example that include manual data/training processing with Pytorch: [Transformers for Multi-Label Classification made simple](https://towardsdatascience.com/transformers-for-multilabel-classification-71a1a0daf5e1)\n",
    "5. [Text Classification with tensorflow](https://github.com/huggingface/notebooks/blob/master/examples/text_classification-tf.ipynb): tensorflow example"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b90e39488bbd77f2537ee13ab7864984f9d6efed60d3b540c631de406366ed67"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
